#!/usr/bin/env python
# coding: utf-8

# # 1. Project : Machine Learning for Health Claim Prediction

# ## 1.1 ‡∏ß‡∏±‡∏ï‡∏ñ‡∏∏‡∏õ‡∏£‡∏∞‡∏™‡∏á‡∏Ñ‡πå‡∏Ç‡∏≠‡∏á‡πÇ‡∏õ‡∏£‡πÄ‡∏à‡∏Ñ

#     üíö ‡∏ß‡∏±‡∏ï‡∏ñ‡∏∏‡∏õ‡∏£‡∏∞‡∏™‡∏á‡∏Ñ‡πå‡∏´‡∏•‡∏±‡∏Å‡∏Ç‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏ó‡∏≥ Health Insurance Claim Prediction ‡πÅ‡∏ö‡∏ö Sepervised Learninig Regression 
#     ‡∏Ñ‡∏∑‡∏≠ ‡∏Å‡∏≤‡∏£‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ó‡∏µ‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏õ‡∏£‡∏∞‡∏Å‡∏±‡∏ô‡∏™‡∏∏‡∏Ç‡∏†‡∏≤‡∏û‡∏Ç‡∏≠‡∏á‡∏ú‡∏π‡πâ‡∏ñ‡∏∑‡∏≠‡∏Å‡∏£‡∏°‡∏ò‡∏£‡∏£‡∏°‡πå‡∏ß‡πà‡∏≤‡∏à‡∏∞‡∏°‡∏µ‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡∏õ‡∏£‡∏∞‡∏°‡∏≤‡∏ì‡πÄ‡∏ó‡πà‡∏≤‡πÑ‡∏´‡∏£‡πà‡∏ï‡πà‡∏≠‡∏£‡∏≠‡∏ö‡∏Å‡∏£‡∏°‡∏ò‡∏£‡∏£‡∏°‡πå

# # 2. Importing Libraries

# ## 1.2 Data Definition

# In[1]:


get_ipython().system('pip install -U imbalanced-learn')


# In[2]:


get_ipython().system('pip  install missingno')


# In[3]:


import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

import missingno as mso

import warnings 
warnings.filterwarnings("ignore")


# In[4]:


from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder


# In[5]:


# For Models
from sklearn.model_selection import train_test_split, KFold , cross_val_score, GridSearchCV
from sklearn.compose import ColumnTransformer
from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet
from sklearn.tree import DecisionTreeRegressor
from sklearn.ensemble import RandomForestRegressor

# For Evaluation 
from sklearn.metrics import r2_score,mean_absolute_error, mean_squared_error, mean_absolute_percentage_error


# # 3. Reading Dataset

# In[6]:


df = pd.read_csv(r"C:\Users\muent\Desktop\my project for data analyst\Project-14_Health_Insurance_Claim_Prediction\Health_Insurance_Dataset\Health_Insurance_Dataset.csv")


# In[7]:


df


# ### Variables :
# 
# 1. age : ‡∏≠‡∏≤‡∏¢‡∏∏‡∏ú‡∏π‡πâ‡∏ñ‡∏∑‡∏≠‡∏Å‡∏£‡∏°‡∏ò‡∏£‡∏£‡∏°‡πå (Numeric)
# 2. sex: ‡πÄ‡∏û‡∏®‡∏ú‡∏π‡πâ‡∏ñ‡∏∑‡∏≠‡∏Å‡∏£‡∏°‡∏ò‡∏£‡∏£‡∏°‡πå (Categoric)
# 3. weight: ‡∏ô‡πâ‡∏≥‡∏´‡∏ô‡∏±‡∏Å‡∏ï‡∏±‡∏ß (Numeric)
# 4. bmi: ‡∏î‡∏±‡∏ä‡∏ô‡∏µ‡∏°‡∏ß‡∏•‡∏Å‡∏≤‡∏¢ (Numeric)
# 5. hereditary_diseases: ‡πÇ‡∏£‡∏Ñ‡∏õ‡∏£‡∏∞‡∏à‡∏≥‡∏ï‡∏±‡∏ß (Categoric)
# 6. no_of_dependents: ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏ú‡∏π‡πâ‡∏≠‡∏¢‡∏π‡πà‡πÉ‡∏ô‡∏≠‡∏∏‡∏õ‡∏Å‡∏≤‡∏£‡∏∞‡∏Ç‡∏≠‡∏á‡∏ú‡∏π‡πâ‡∏ñ‡∏∑‡∏≠‡∏Å‡∏£‡∏°‡∏ò‡∏£‡∏£‡∏°‡πå (Numeric)
# 7. smoker: ‡∏Å‡∏≤‡∏£‡∏™‡∏π‡∏ö‡∏ö‡∏∏‡∏´‡∏£‡∏µ‡πà (non-smoker=0;smoker=1) (Categoric)
# 8. city: ‡πÄ‡∏°‡∏∑‡∏≠‡∏á‡∏ó‡∏µ‡πà‡∏≠‡∏≤‡∏®‡∏±‡∏¢ (Categoric)
# 9. bloodpressure: ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏î‡∏±‡∏ô‡πÇ‡∏•‡∏´‡∏¥‡∏ï (Numeric)
# 10. diabetes: ‡πÇ‡∏£‡∏Ñ‡πÄ‡∏ö‡∏≤‡∏´‡∏ß‡∏≤‡∏ô (non-diabetic=0; diabetic=1) (Categoric)
# 11. regular_ex: ‡∏Å‡∏≤‡∏£‡∏≠‡∏≠‡∏Å‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏Å‡∏≤‡∏¢ (no-excercise=0; excercise=1) (Categoric)
# 12. job_title: ‡∏≠‡∏≤‡∏ä‡∏µ‡∏û (Categoric)
# 13. claim: ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡πÄ‡∏á‡∏¥‡∏ô‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡∏£‡πâ‡∏≠‡∏á‡∏Ñ‡πà‡∏≤‡∏™‡∏¥‡∏ô‡πÑ‡∏´‡∏°‡∏õ‡∏£‡∏∞‡∏Å‡∏±‡∏ô‡∏™‡∏∏‡∏Ç‡∏†‡∏≤‡∏û (Numeric)

# In[8]:


df.info()


# In[9]:


df.columns


# # 4. Data Exploration(EDA)

# **‡∏ï‡∏±‡∏ß‡πÅ‡∏õ‡∏£‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡∏´‡∏°‡∏ß‡∏î‡∏´‡∏°‡∏π‡πà (Categorical Variable)**
# - ‡πÑ‡∏î‡πâ‡πÅ‡∏Å‡πà sex, hereditary_diseases, city, job_title, smoker, diabetes, regular_ex 
# 
# **‡∏ï‡∏±‡∏ß‡πÅ‡∏õ‡∏£‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç (Numerical Variable)**
# - ‡πÑ‡∏î‡πâ‡πÅ‡∏Å‡πà age, weight, bmi, no_of_dependents, bloodpressure, claim

# ## 4.1 Categorical Variable

# ### 4.1.1 Sex

# In[10]:


df.sex.value_counts(dropna= False)


# In[11]:


sns.countplot(data = df , x = "sex", palette= "Paired")
plt.show()


# In[12]:


countMale = len(df[df.sex == "male"])
countFemale = len(df[df.sex == "female"])
countNull = len(df[df.sex.isnull()])

print("Percantage of Male policyholder  : {:.2f}%".format(countMale*100/len(df.sex)))
print("Percantage of Female policyholder  : {:.2f}%".format(countFemale*100/len(df.sex)))
print("Missing values percentage: {:.2f}%".format(countNull*100/len(df.sex)))


#     üíö ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏ú‡∏π‡πâ‡∏ñ‡∏∑‡∏≠‡∏Å‡∏£‡∏°‡∏ò‡∏£‡∏£‡∏°‡πå‡πÄ‡∏û‡∏®‡∏´‡∏ç‡∏¥‡∏á‡∏™‡∏π‡∏á‡∏Å‡∏ß‡πà‡∏≤‡πÄ‡∏û‡∏®‡∏ä‡∏≤‡∏¢
#     ‚úÖ ‡πÉ‡∏ô‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏ô‡∏µ‡πâ‡πÑ‡∏°‡πà‡∏°‡∏µ Missing Value 

# ### 4.1.2 Hereditary_diseases

# In[13]:


df.hereditary_diseases.value_counts(dropna= False)


# In[14]:


sns.countplot(data = df , y = "hereditary_diseases", palette= "hls", order=df['hereditary_diseases'].value_counts().index )
plt.show()


# In[15]:


df.hereditary_diseases.unique()


# In[16]:


countNoDisease = len(df[df.hereditary_diseases == "NoDisease"])
countDiabetes = len(df[df.hereditary_diseases == "Diabetes"])
countAlzheimer = len(df[df.hereditary_diseases == "Alzheimer"])
countObesity = len(df[df.hereditary_diseases == "Obesity"])
countEyeDisease = len(df[df.hereditary_diseases == "EyeDisease"])
countCancer = len(df[df.hereditary_diseases == "Cancer"])
countArthritis = len(df[df.hereditary_diseases == "Arthritis"])
countHeartDisease = len(df[df.hereditary_diseases == "HeartDisease"])
countEpilepsy = len(df[df.hereditary_diseases == "Epilepsy"])
countHighBP = len(df[df.hereditary_diseases == "High BP"])
countNull = len(df[df.hereditary_diseases.isnull()])

print("Percantage of NoDisease  : {:.2f}%".format(countNoDisease*100/len(df.hereditary_diseases)))
print("Percantage of Diabetes  : {:.2f}%".format(countDiabetes*100/len(df.hereditary_diseases)))
print("Percantage of Alzheimer  : {:.2f}%".format(countAlzheimer*100/len(df.hereditary_diseases)))
print("Percantage of Obesity  : {:.2f}%".format(countObesity*100/len(df.hereditary_diseases)))
print("Percantage of EyeDisease  : {:.2f}%".format(countEyeDisease *100/len(df.hereditary_diseases)))
print("Percantage of Cancer  : {:.2f}%".format(countCancer*100/len(df.hereditary_diseases)))
print("Percantage of Arthritis  : {:.2f}%".format(countArthritis*100/len(df.hereditary_diseases)))
print("Percantage of HeartDisease  : {:.2f}%".format(countHeartDisease*100/len(df.hereditary_diseases)))
print("Percantage of Epilepsy  : {:.2f}%".format(countEpilepsy*100/len(df.hereditary_diseases)))
print("Percantage of High BP  : {:.2f}%".format(countHighBP*100/len(df.hereditary_diseases)))
print("Missing values percentage: {:.2f}%".format(countNull*100/len(df.hereditary_diseases)))


#     üíö ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏ú‡∏π‡πâ‡∏ñ‡∏∑‡∏≠‡∏Å‡∏£‡∏°‡∏ò‡∏£‡∏£‡∏°‡πå‡∏™‡πà‡∏ß‡∏ô‡∏°‡∏≤‡∏Å‡πÑ‡∏°‡πà‡∏°‡∏µ‡πÇ‡∏£‡∏Ñ‡∏õ‡∏£‡∏∞‡∏à‡∏≥‡∏ï‡∏±‡∏ß
#     ‚úÖ ‡πÉ‡∏ô‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏ô‡∏µ‡πâ‡πÑ‡∏°‡πà‡∏°‡∏µ Missing Value 

# ### 4.1.3 Smoker

# In[17]:


df.smoker.value_counts(dropna= False)


# In[18]:


sns.countplot(data = df , x = "smoker", palette= "cubehelix")
plt.show()


# In[19]:


df.dtypes


# In[20]:


countNoSmoker= len(df[df.smoker == 0])
countSmoker = len(df[df.smoker == 1])
countNull = len(df[df.smoker.isnull()])

print("Percantage of No Smoker  : {:.2f}%".format(countNoSmoker*100/len(df.smoker)))
print("Percantage of Smoker  : {:.2f}%".format(countSmoker*100/len(df.smoker)))
print("Missing values percentage: {:.2f}%".format(countNull*100/len(df.smoker)))


#     üíö ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏ú‡∏π‡πâ‡∏ñ‡∏∑‡∏≠‡∏Å‡∏£‡∏°‡∏ò‡∏£‡∏£‡∏°‡πå‡∏™‡πà‡∏ß‡∏ô‡∏°‡∏≤‡∏Å‡πÑ‡∏°‡πà‡∏™‡∏π‡∏ö‡∏ö‡∏∏‡∏´‡∏£‡∏µ‡πà
#     ‚úÖ ‡πÉ‡∏ô‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏ô‡∏µ‡πâ‡πÑ‡∏°‡πà‡∏°‡∏µ Missing Value 

# ### 4.1.4 City

# In[21]:


df.city.value_counts(dropna=False)


# In[22]:


city_counts = df['city'].value_counts()

# 10 ‡πÄ‡∏°‡∏∑‡∏≠‡∏á‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏ú‡∏π‡πâ‡∏ñ‡∏∑‡∏≠‡∏Å‡∏£‡∏°‡∏ò‡∏£‡∏£‡∏°‡πå‡∏™‡πà‡∏ß‡∏ô‡∏°‡∏≤‡∏Å
top_cities = city_counts.head(10)

# ‡∏£‡∏ß‡∏°‡∏™‡πà‡∏ß‡∏ô‡∏ó‡∏µ‡πà‡πÄ‡∏´‡∏•‡∏∑‡∏≠‡πÄ‡∏õ‡πá‡∏ô 'Other'
other = city_counts.sum() - top_cities.sum()
top_cities['Other'] = other

# ‡∏™‡∏£‡πâ‡∏≤‡∏á pie chart
plt.figure(figsize=(8, 6))
top_cities.plot.pie(autopct='%1.1f%%')
plt.axis('equal')  # ‡∏ó‡∏≥‡πÉ‡∏´‡πâ pie chart ‡πÄ‡∏õ‡πá‡∏ô‡∏ß‡∏á‡∏Å‡∏•‡∏°
plt.title('Top 10 Cities and Others')
plt.show()


#     üíö ‡πÅ‡∏™‡∏î‡∏á‡πÄ‡∏°‡∏∑‡∏≠‡∏á‡∏ó‡∏µ‡πà‡∏ú‡∏π‡πâ‡∏ñ‡∏∑‡∏≠‡∏Å‡∏£‡∏°‡∏ò‡∏£‡∏£‡∏°‡πå‡∏≠‡∏¢‡∏π‡πà‡∏≠‡∏≤‡∏®‡∏±‡∏¢
#     ‚úÖ ‡πÉ‡∏ô‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏ô‡∏µ‡πâ‡πÑ‡∏°‡πà‡∏°‡∏µ Missing Value 

# ### 4.1.5 diabetes

# In[23]:


df.diabetes.value_counts(dropna= False)

# 0 Non diabetic
# 1 Diabetic


# In[24]:


sns.countplot(data = df , x = "diabetes", palette= "hls")
plt.show()


# In[25]:


countNoDiabete= len(df[df.diabetes == 0])
countDiabete = len(df[df.diabetes == 1])
countNull = len(df[df.diabetes.isnull()])

print("Percantage of No Diabete  : {:.2f}%".format(countNoDiabete*100/len(df.diabetes)))
print("Percantage of Diabete  : {:.2f}%".format(countDiabete*100/len(df.diabetes)))
print("Missing values percentage: {:.2f}%".format(countNull*100/len(df.diabetes)))


#     üíö ‡∏ú‡∏π‡πâ‡∏ñ‡∏∑‡∏≠‡∏Å‡∏£‡∏°‡∏ò‡∏£‡∏£‡∏°‡πå‡πÄ‡∏õ‡πá‡∏ô‡πÇ‡∏£‡∏Ñ‡πÄ‡∏ö‡∏≤‡∏´‡∏ß‡∏≤‡∏ô‡∏™‡∏π‡∏á‡∏Å‡∏ß‡πà‡∏≤‡∏ú‡∏π‡πâ‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡πÄ‡∏õ‡πá‡∏ô‡πÇ‡∏£‡∏Ñ‡πÄ‡∏ö‡∏≤‡∏´‡∏ß‡∏≤‡∏ô
#     ‚úÖ ‡πÉ‡∏ô‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏ô‡∏µ‡πâ‡πÑ‡∏°‡πà‡∏°‡∏µ Missing Value 

# ### 4.1.6 Regular Exercise

# In[26]:


df.regular_ex.value_counts(dropna= False)

# 0 No Exercise
# 1 Exercise


# In[27]:


sns.countplot(data = df , x = "regular_ex", palette= "crest")
plt.show()


# In[28]:


countNoExercise= len(df[df.regular_ex == 0])
countExercise = len(df[df.regular_ex == 1])
countNull = len(df[df.regular_ex.isnull()])

print("Percantage of No Exercise : {:.2f}%".format(countNoExercise*100/len(df.regular_ex)))
print("Percantage of Exercise  : {:.2f}%".format(countExercise*100/len(df.regular_ex)))
print("Missing values percentage: {:.2f}%".format(countNull*100/len(df.regular_ex)))


#     üíö ‡∏ú‡∏π‡πâ‡∏ñ‡∏∑‡∏≠‡∏Å‡∏£‡∏°‡∏ò‡∏£‡∏£‡∏°‡πå‡πÑ‡∏°‡πà‡∏ä‡∏≠‡∏ö‡∏≠‡∏≠‡∏Å‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏Å‡∏≤‡∏¢‡∏°‡∏µ‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏™‡∏π‡∏á‡∏Å‡∏ß‡πà‡∏≤‡∏ú‡∏π‡πâ‡∏ó‡∏µ‡πà‡∏≠‡∏≠‡∏Å‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏Å‡∏≤‡∏¢
#     ‚úÖ ‡πÉ‡∏ô‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏ô‡∏µ‡πâ‡πÑ‡∏°‡πà‡∏°‡∏µ Missing Value 

# ### 4.1.7 Job Title

# In[29]:


df.job_title.value_counts(dropna= False) 


# In[30]:


countNull = len(df[df.job_title.isnull()])
print("Missing values percentage: {:.2f}%".format(countNull*100/len(df.job_title)))


#     üíö  ‡πÅ‡∏™‡∏î‡∏á‡∏≠‡∏≤‡∏ä‡∏µ‡∏û‡∏Ç‡∏≠‡∏á‡∏ú‡∏π‡πâ‡∏ñ‡∏∑‡∏≠‡∏Å‡∏£‡∏°‡∏ò‡∏£‡∏£‡∏°‡πå
#     ‚úÖ ‡πÉ‡∏ô‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏ô‡∏µ‡πâ‡πÑ‡∏°‡πà‡∏°‡∏µ Missing Value 

# ## 4.2 Numerical Variable

# **‡∏ï‡∏±‡∏ß‡πÅ‡∏õ‡∏£‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç (Numerical Variable)**
# - ‡πÑ‡∏î‡πâ‡πÅ‡∏Å‡πà age, weight, bmi, no_of_dependents, bloodpressure, claim

# ### 4.2.1 Describe Numerical Variable (‡∏™‡∏ñ‡∏¥‡∏ï‡∏¥‡πÄ‡∏ö‡∏∑‡πâ‡∏≠‡∏á‡∏ï‡πâ‡∏ô)

# In[31]:


df


# In[32]:


df[["age", "weight", "bmi", "no_of_dependents", "bloodpressure", "claim"]].describe()


# ### 4.2.2 Distribution of Numerical Variable

# üíöüß° ‡∏à‡∏∞‡∏û‡∏•‡πá‡∏≠‡∏ï‡∏Å‡∏£‡∏≤‡∏ü Histogram  ‡πÅ‡∏•‡∏∞ Box plot ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏û‡∏¥‡∏à‡∏≤‡∏£‡∏ì‡∏≤‡∏Å‡∏≤‡∏£‡∏Å‡∏£‡∏∞‡∏à‡∏≤‡∏¢‡∏ï‡∏±‡∏ß‡πÅ‡∏•‡∏∞ ‡∏î‡∏π outlier

# #### 4.2.2.1 Histogram Distribution üíöüíõ ‡∏Å‡∏£‡∏≤‡∏ü‡πÅ‡∏™‡∏î‡∏á‡∏Å‡∏≤‡∏£‡∏Å‡∏£‡∏∞‡∏à‡∏≤‡∏¢‡∏ï‡∏±‡∏ß‡∏Ç‡∏≠‡∏á‡∏ï‡∏±‡∏ß‡πÅ‡∏õ‡∏£‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó numerical variable

# In[33]:


sns.set(style="darkgrid")
fig, axs = plt.subplots(2, 3, figsize=(15, 8), sharex=False)  # sharex=False ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÑ‡∏°‡πà‡πÉ‡∏´‡πâ‡πÅ‡∏Å‡∏ô x ‡∏Ç‡∏≠‡∏á‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏Å‡∏£‡∏≤‡∏ü‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡πÇ‡∏¢‡∏á‡∏Å‡∏±‡∏ô

sns.histplot(data=df, x="age", kde=True, ax=axs[0, 0], color='green')
sns.histplot(data=df, x="weight", kde=True, ax=axs[0, 1], color='skyblue')
sns.histplot(data=df, x="bmi", kde=True, ax=axs[0, 2], color='orange')
sns.histplot(data=df, x="no_of_dependents", kde=True, ax=axs[1, 0], color='purple')
sns.histplot(data=df, x="bloodpressure", kde=True, ax=axs[1, 1], color='blue')
sns.histplot(data=df, x="claim", kde=True, ax=axs[1, 2], color='red')

# ‡∏õ‡∏£‡∏±‡∏ö‡∏ä‡πà‡∏≠‡∏á‡∏ß‡πà‡∏≤‡∏á‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á‡∏Å‡∏£‡∏≤‡∏ü‡∏î‡πâ‡∏≤‡∏ô‡∏ö‡∏ô‡πÅ‡∏•‡∏∞‡∏î‡πâ‡∏≤‡∏ô‡∏•‡πà‡∏≤‡∏á
fig.subplots_adjust(hspace=0.3 ,  wspace= 0.3)  


# #### 4.2.2.1 Boxplot üíöüíõ ‡∏Å‡∏£‡∏≤‡∏ü‡πÅ‡∏™‡∏î‡∏á outlier

# In[34]:


sns.set(style="darkgrid")
fig, axs = plt.subplots(2, 3, figsize=(15, 8), sharex=False)  # sharex=False ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÑ‡∏°‡πà‡πÉ‡∏´‡πâ‡πÅ‡∏Å‡∏ô x ‡∏Ç‡∏≠‡∏á‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏Å‡∏£‡∏≤‡∏ü‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡πÇ‡∏¢‡∏á‡∏Å‡∏±‡∏ô

sns.boxplot(data=df, y="age",ax=axs[0, 0], color='green')
sns.boxplot(data=df, y="weight",  ax=axs[0, 1], color='skyblue')
sns.boxplot(data=df, y="bmi",  ax=axs[0, 2], color='orange')
sns.boxplot(data=df, y="no_of_dependents",  ax=axs[1, 0], color='purple')
sns.boxplot(data=df, y="bloodpressure", ax=axs[1, 1], color='blue')
sns.boxplot(data=df, y="claim",  ax=axs[1, 2], color='red')

# ‡∏õ‡∏£‡∏±‡∏ö‡∏ä‡πà‡∏≠‡∏á‡∏ß‡πà‡∏≤‡∏á‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á‡∏Å‡∏£‡∏≤‡∏ü‡∏î‡πâ‡∏≤‡∏ô‡∏ö‡∏ô‡πÅ‡∏•‡∏∞‡∏î‡πâ‡∏≤‡∏ô‡∏•‡πà‡∏≤‡∏á
fig.subplots_adjust(hspace=0.3 ,  wspace= 0.3)  


# - ‡∏Å‡∏≤‡∏£‡∏Å‡∏£‡∏∞‡∏à‡∏≤‡∏¢‡∏ï‡∏±‡∏ß‡∏Ç‡∏≠‡∏á age, weight, bmi, no_of_dependents, bloodpressure, claim ‡πÄ‡∏õ‡πá‡∏ô‡πÅ‡∏ö‡∏ö‡∏Å‡∏£‡∏≤‡∏ü‡πÄ‡∏ö‡πâ 
# - ‡πÅ‡∏•‡∏∞ bmi, bloodpressure, claim  ‡∏°‡∏µ‡∏Ñ‡πà‡∏≤ outlier ‡∏î‡πâ‡∏ß‡∏¢ 

# ## 4.3 Correlation (‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏±‡∏°‡∏û‡∏±‡∏ô‡∏ò‡πå‡∏Ç‡∏≠‡∏á‡∏ï‡∏±‡∏ß‡πÅ‡∏õ‡∏£)

# In[35]:


df[["age", "weight", "bmi", "no_of_dependents", "bloodpressure", "claim"]].corr()


# In[36]:


plt.figure(figsize=(6,4))
sns.heatmap(df[["age", "weight", "bmi", "no_of_dependents", "bloodpressure", "claim"]].corr(), annot= True, cmap='viridis')
plt.show()


# #### üíõüíöüß° ‡∏•‡∏±‡∏Å‡∏©‡∏ì‡∏∞‡∏Ç‡∏≠‡∏á Correlation ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ä‡∏∏‡∏î‡∏ô‡∏µ‡πâ‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏±‡∏°‡∏û‡∏±‡∏ô‡∏ò‡πå‡πÅ‡∏ö‡∏ö Non-linear

# In[37]:


sns.pairplot(df, 
             vars = ["age", "weight", "bmi", "no_of_dependents", "bloodpressure", "claim"],
            hue = "sex")
plt.show()


# #### üíõüíöüß° ‡∏•‡∏±‡∏Å‡∏©‡∏ì‡∏∞‡∏Ç‡∏≠‡∏á Pair Plots ‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ä‡∏∏‡∏î‡∏ô‡∏µ‡πâ‡πÄ‡∏õ‡πá‡∏ô‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏±‡∏°‡∏û‡∏±‡∏ô‡∏ò‡πå‡πÅ‡∏ö‡∏ö Non-linear

# ## 4.4 Handle missing values

# In[38]:


df.isnull().sum()


# In[39]:


sns.heatmap(df.isnull())
plt.show()


#     üß° ‡πÅ‡∏™‡∏î‡∏á‡∏Ñ‡πà‡∏≤ null ‡∏Ç‡∏≠‡∏á‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏ï‡∏±‡∏ß‡πÅ‡∏õ‡∏£
#     üß° age ‡πÅ‡∏•‡∏∞ bmi ‡∏°‡∏µ missing values

# # 5. Data Preprocessing

# ## 5.2 Data Imputation ‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡∏°‡∏≤‡∏ì‡∏Ñ‡πà‡∏≤‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡∏ï‡∏±‡∏ß‡πÅ‡∏õ‡∏£‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏Ñ‡πà‡∏≤‡∏´‡∏≤‡∏¢‡πÑ‡∏õ(Missing Values)

#     üíö ‡∏ï‡∏±‡∏ß‡πÅ‡∏õ‡∏£‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏Ñ‡πà‡∏≤‡∏´‡∏≤‡∏¢‡πÑ‡∏õ(Missing Values) ‡∏Ñ‡∏∑‡∏≠ age ‡πÅ‡∏•‡∏∞ bmi ‡∏ã‡∏∂‡πà‡∏á‡πÄ‡∏õ‡πá‡∏ô‡∏ï‡∏±‡∏ß‡πÅ‡∏õ‡∏£‡∏õ‡∏£‡∏∞‡πÄ‡∏†‡∏ó Numerical variable 
#     üíö ‡πÉ‡∏ô‡∏™‡πà‡∏ß‡∏ô‡∏Ç‡∏≠‡∏á‡∏ï‡∏±‡∏ß‡πÅ‡∏õ‡∏£‡∏ä‡∏ô‡∏¥‡∏î Numerical Variables ‡∏à‡∏∞‡πÅ‡∏ó‡∏ô‡∏Ñ‡πà‡∏≤‡∏ó‡∏µ‡πà‡∏´‡∏≤‡∏¢‡πÑ‡∏õ‡∏î‡πâ‡∏ß‡∏¢ ‡∏Ñ‡πà‡∏≤ "mean"

# In[40]:


# coumns ‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô Numerical Variables   ‡∏ó‡∏µ‡πà‡∏°‡∏µ missing values
# age ‡πÅ‡∏•‡∏∞ bmi 

df.select_dtypes(include=['int64', 'float64']).isnull().sum()


# In[41]:


df["age"].fillna(df["age"].mean(), inplace = True)
df["bmi"].fillna(df["bmi"].mean(), inplace = True)


# ## 5.3 One-hot Encoding

#     üß° ‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏£‡∏´‡∏±‡∏™ encoding ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•

# In[42]:


df.head()


# In[43]:


df = pd.get_dummies(df, drop_first=True)


# In[44]:


df


# ## 5.4 Handle Outliers

# #### üíõ ‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡∏Ñ‡πà‡∏≤ Outlier

#     üíõ ‡πÉ‡∏ä‡πâ‡∏ß‡∏¥‡∏ò‡∏µ Interquartile Range (IQR) : ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì IQR (Q3-Q1) ‡πÅ‡∏•‡∏∞‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏Ç‡∏≠‡∏ö‡πÄ‡∏Ç‡∏ï (Q1-1.5IQR) ‡πÅ‡∏•‡∏∞ (Q3+1.5IQR)
#     üíõ ‡πÉ‡∏ä‡πâ Clip: ‡πÄ‡∏û‡∏£‡∏≤‡∏∞‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏£‡∏±‡∏Å‡∏©‡∏≤‡∏à‡∏≥‡∏ô‡∏ß‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÑ‡∏ß‡πâ ‡πÅ‡∏ï‡πà‡∏•‡∏î‡∏ú‡∏•‡∏Å‡∏£‡∏∞‡∏ó‡∏ö‡∏Ç‡∏≠‡∏á Outliers

# - Feature ‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏Ñ‡πà‡∏≤ outlier ‡∏Ñ‡∏∑‡∏≠ **bmi** ,**bloodpressure** ‡πÅ‡∏•‡∏∞ **claim**

# In[45]:


sns.set(style="darkgrid")
fig, axs = plt.subplots(2, 2, figsize=(8, 6), sharex=False)  # sharex=False ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÑ‡∏°‡πà‡πÉ‡∏´‡πâ‡πÅ‡∏Å‡∏ô x ‡∏Ç‡∏≠‡∏á‡πÅ‡∏ï‡πà‡∏•‡∏∞‡∏Å‡∏£‡∏≤‡∏ü‡πÄ‡∏ä‡∏∑‡πà‡∏≠‡∏°‡πÇ‡∏¢‡∏á‡∏Å‡∏±‡∏ô

sns.boxplot(data=df, y="bmi",  ax=axs[0, 0], color='orange')
sns.boxplot(data=df, y="bloodpressure", ax=axs[0, 1], color='blue')
sns.boxplot(data=df, y="claim",  ax=axs[1, 0], color='red')

fig.suptitle("Before handling outliers", fontsize=16, fontweight='bold')
fig.subplots_adjust(hspace=0.3 ,  wspace= 0.3) 


# In[46]:


# ‡∏Ñ‡∏≠‡∏•‡∏±‡∏°‡∏ô‡πå‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£ outliers
columns_to_handle = ['bmi', 'bloodpressure', 'claim']

# ‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£ outliers ‡πÉ‡∏ô columns ‡∏ó‡∏µ‡πà‡∏£‡∏∞‡∏ö‡∏∏‡πÇ‡∏î‡∏¢‡πÉ‡∏ä‡πâ IQR
for col in columns_to_handle:
    Q1 = df[col].quantile(0.25)
    Q3 = df[col].quantile(0.75)
    IQR = Q3 - Q1
    lower_bound = Q1 - 1.5 * IQR
    upper_bound = Q3 + 1.5 * IQR
    df[col] = np.clip(df[col], lower_bound, upper_bound)


# In[47]:


sns.set(style="darkgrid")
fig, axs = plt.subplots(2, 2, figsize=(8, 6), sharex=False)  

sns.boxplot(data=df, y="bmi",  ax=axs[0, 0], color='orange')
sns.boxplot(data=df, y="bloodpressure", ax=axs[0, 1], color='blue')
sns.boxplot(data=df, y="claim",  ax=axs[1, 0], color='red')

fig.suptitle("After handling outliers", fontsize=16, fontweight='bold')
fig.subplots_adjust(hspace=0.3 ,  wspace= 0.3) 


# ## 5.5 Skewed Distribution Treatment

# ##### üíõüíö ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ó‡∏≥‡πÉ‡∏´‡πâ‡∏Å‡∏£‡∏≤‡∏ü‡πÄ‡∏ö‡πâ‡∏Ç‡∏ß‡∏≤‡πÉ‡∏´‡πâ‡πÉ‡∏Å‡∏•‡πâ‡πÄ‡∏Ñ‡∏µ‡∏¢‡∏á‡∏Å‡∏≤‡∏£‡∏Å‡∏£‡∏∞‡∏à‡∏≤‡∏¢‡∏ï‡∏±‡∏ß‡πÅ‡∏ö‡∏ö normalized  distribution
# - ‡∏Å‡∏≤‡∏£‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡∏Å‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏Å‡∏£‡∏∞‡∏à‡∏≤‡∏¢‡∏ï‡∏±‡∏ß‡∏Ç‡∏≠‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡πÄ‡∏ö‡πâ (Skewed Distribution) ‡∏î‡πâ‡∏ß‡∏¢‡∏ß‡∏¥‡∏ò‡∏µ Square Root Transformation
# - ‡πÄ‡∏õ‡πá‡∏ô‡∏´‡∏ô‡∏∂‡πà‡∏á‡πÉ‡∏ô‡πÄ‡∏ó‡∏Ñ‡∏ô‡∏¥‡∏Ñ‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ‡∏Å‡∏±‡∏ô‡∏ö‡πà‡∏≠‡∏¢‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏±‡∏ö‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡∏Ç‡∏≠‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏´‡πâ‡πÉ‡∏Å‡∏•‡πâ‡πÄ‡∏Ñ‡∏µ‡∏¢‡∏á‡∏Å‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡πÅ‡∏à‡∏Å‡πÅ‡∏à‡∏á‡πÅ‡∏ö‡∏ö‡∏õ‡∏Å‡∏ï‡∏¥ (Normal Distribution) ‡∏°‡∏≤‡∏Å‡∏Ç‡∏∂‡πâ‡∏ô 
# - ‡∏ß‡∏¥‡∏ò‡∏µ‡∏ô‡∏µ‡πâ‡∏à‡∏∞‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏Ñ‡πà‡∏≤‡πÄ‡∏ö‡πâ‡πÑ‡∏õ‡∏ó‡∏≤‡∏á‡∏Ç‡∏ß‡∏≤ (Right-Skewed)
# 
# ***‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏ï‡∏¥‡∏°***
# - ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏Å‡∏≤‡∏£‡∏Å‡∏£‡∏∞‡∏à‡∏≤‡∏¢‡∏ï‡∏±‡∏ß‡πÅ‡∏ö‡∏ö‡πÄ‡∏ö‡πâ‡πÑ‡∏õ‡∏ó‡∏≤‡∏á‡∏ã‡πâ‡∏≤‡∏¢ (Left-Skewed) ‡∏ô‡∏±‡πâ‡∏ô ‡∏ß‡∏¥‡∏ò‡∏µ‡∏ó‡∏µ‡πà‡∏ô‡∏¥‡∏¢‡∏°‡πÉ‡∏ä‡πâ‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏±‡∏ö‡πÅ‡∏Å‡πâ‡∏Ñ‡∏∑‡∏≠ Cube Root Transformation
# - Cube Root Transformation ‡∏Ñ‡∏∑‡∏≠‡∏Å‡∏≤‡∏£‡∏´‡∏≤‡∏£‡∏≤‡∏Å‡∏ó‡∏µ‡πà‡∏™‡∏≤‡∏°‡∏Ç‡∏≠‡∏á‡∏Ñ‡πà‡∏≤‡πÉ‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ô‡∏±‡πâ‡∏ô
# - ‡∏ô‡∏≠‡∏Å‡∏à‡∏≤‡∏Å‡∏ô‡∏µ‡πâ ‡∏¢‡∏±‡∏á‡∏°‡∏µ‡∏ß‡∏¥‡∏ò‡∏µ‡∏≠‡∏∑‡πà‡∏ô‡πÜ ‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£ Skewed Distribution 
# - ‡πÄ‡∏ä‡πà‡∏ô Log Transformation, Box-Cox Transformation ‡πÄ‡∏õ‡πá‡∏ô‡∏ï‡πâ‡∏ô 

# In[48]:


sns.set(style="darkgrid")
fig, axs = plt.subplots(2, 2, figsize=(8, 6), sharex= False)

sns.histplot(data=df, x="weight", kde=True, ax=axs[0, 0], color='skyblue')
sns.histplot(data=df, x="bmi", kde=True, ax=axs[0, 1], color='orange')
sns.histplot(data=df, x="bloodpressure", kde=True, ax=axs[1, 0], color='blue')
sns.histplot(data=df, x="claim", kde=True, ax=axs[1, 1], color='red')

fig.suptitle("Before handling Skewed Distribution Treatment", fontsize=16, fontweight='bold')
fig.subplots_adjust(hspace= 0.3 ,wspace=0.3);


# In[49]:


# Square Root Transformation 

df.weight = np.sqrt(df.weight)
df.bmi = np.sqrt(df.bmi)
df.bloodpressure = np.sqrt(df.bloodpressure)
df.claim = np.sqrt(df.claim)


# In[50]:


sns.set(style="darkgrid")
fig, axs = plt.subplots(2, 2, figsize=(8, 6), sharex= False)

sns.histplot(data=df, x="weight", kde=True, ax=axs[0, 0], color='skyblue')
sns.histplot(data=df, x="bmi", kde=True, ax=axs[0, 1], color='orange')
sns.histplot(data=df, x="bloodpressure", kde=True, ax=axs[1, 0], color='blue')
sns.histplot(data=df, x="claim", kde=True, ax=axs[1, 1], color='red')

fig.suptitle("After handling Skewed Distribution Treatment", fontsize=16, fontweight='bold')
fig.subplots_adjust(hspace= 0.3 ,wspace=0.3);


# ## 5.6 Features Separating and Drop Unecessary Variables

# üß°üíö ‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏ï‡∏±‡∏ß‡πÅ‡∏õ‡∏£‡∏≠‡∏¥‡∏™‡∏£‡∏∞ (X) ‡πÅ‡∏•‡∏∞ ‡∏ï‡∏±‡∏ß‡πÅ‡∏õ‡∏£‡∏ï‡∏≤‡∏°(y)

# In[51]:


X = df.drop(["claim"], axis=1)
y = df["claim"]


# In[52]:


X


# In[53]:


y


# In[54]:


# --- ‡∏Å‡πà‡∏≠‡∏ô‡πÅ‡∏ö‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏• train/test ‡πÉ‡∏´‡πâ‡πÄ‡∏Å‡πá‡∏ö‡∏ä‡∏∑‡πà‡∏≠ column ‡πÑ‡∏ß‡πâ ---
feature_names = X.columns  # ‡∏™‡∏°‡∏°‡∏ï‡∏¥‡∏ß‡πà‡∏≤ X ‡πÄ‡∏õ‡πá‡∏ô DataFrame ‡∏Å‡πà‡∏≠‡∏ô‡πÅ‡∏ö‡πà‡∏á‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
feature_names


# ## 5.7 Data Normalization

# In[55]:


X = StandardScaler().fit_transform(X)


# In[56]:


X


# # 6. ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÅ‡∏•‡∏∞‡πÄ‡∏ó‡∏£‡∏ô‡πÇ‡∏°‡πÄ‡∏î‡∏• (Model Building and Training)

# ## 6.1 Splitting Dataset : 80% train set ‡πÅ‡∏•‡∏∞ 20% test set

# In[57]:


X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 5)


# ## 6.2 ‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÅ‡∏ö‡∏ö‡∏ï‡πà‡∏≤‡∏á

# **‡∏Ñ‡∏≥‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢:  ‡∏à‡∏∞‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÇ‡∏°‡πÄ‡∏î‡∏• 2 ‡πÅ‡∏ö‡∏ö ‡∏ô‡∏±‡πà‡∏ô‡∏Ñ‡∏∑‡∏≠**
# *  **Linear model** ‡πÑ‡∏î‡πâ‡πÅ‡∏Å‡πà Multiple linear regression, Ridge, Lasso, ElasticNet
# *  **Non-linear model** ‡πÑ‡∏î‡πâ‡πÅ‡∏Å‡πà Decision tree regression, Random forest regression

# **‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏á‡∏≤‡∏ô:**
# 
#   1. **Nested Cross-Validation:** 
#         * ‡∏ó‡∏≥  nested cross-validation  ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏´‡∏≤  hyperparameters  ‡∏ó‡∏µ‡πà‡∏î‡∏µ‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î  ‡πÇ‡∏î‡∏¢‡πÉ‡∏ä‡πâ  grid search  ‡πÉ‡∏ô  inner loop  
#         * ‡πÄ‡∏Å‡πá‡∏ö‡∏Ñ‡πà‡∏≤  Average R-squared (Validation Set)  ‡∏à‡∏≤‡∏Å  outer loop 
#   2. **‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡∏ú‡∏• Overfitting:** 
#         * train  ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ó‡∏µ‡πà  train  ‡∏î‡πâ‡∏ß‡∏¢  best parameters  ‡∏à‡∏≤‡∏Å  nested cross-validation  ‡∏ö‡∏ô  training set  ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î  
#         * ‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö  R-squared   ‡∏ö‡∏ô  training set  ‡∏Å‡∏±‡∏ö  Average R-squared (validation set)  ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÄ‡∏ä‡πá‡∏Ñ  overfitting   
#         *  ‡∏ñ‡πâ‡∏≤‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏¢‡∏±‡∏á  overfitting  ‡∏≠‡∏¢‡∏π‡πà  ‡πÉ‡∏´‡πâ‡∏•‡∏≠‡∏á‡∏õ‡∏£‡∏±‡∏ö  hyperparameters  ‡∏´‡∏£‡∏∑‡∏≠‡πÄ‡∏ó‡∏Ñ‡∏ô‡∏¥‡∏Ñ‡∏≠‡∏∑‡πà‡∏ô‡πÜ  
#   3. **‡πÄ‡∏•‡∏∑‡∏≠‡∏Å Best Model:**  
#         *  ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ó‡∏µ‡πà‡∏°‡∏µ  Average R-squared (Validation Set)  ‡∏™‡∏π‡∏á‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î  
#   4. **Train Final Model:** 
#         *  train  ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ó‡∏µ‡πà‡∏î‡∏µ‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î‡∏ö‡∏ô  training set  ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î  ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡πÑ‡∏î‡πâ  final model 
#   5. **‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡∏ú‡∏•‡∏ö‡∏ô Test Set:**  
#         *  ‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡∏ú‡∏•  final model   ‡∏ö‡∏ô  test set  ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏î‡∏π‡∏õ‡∏£‡∏∞‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡∏†‡∏≤‡∏û‡∏™‡∏∏‡∏î‡∏ó‡πâ‡∏≤‡∏¢‡∏Å‡∏±‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÉ‡∏´‡∏°‡πà 

# **‡∏Ñ‡∏≥‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢:**
# * model_cv: ‡πÉ‡∏ä‡πâ‡πÄ‡∏â‡∏û‡∏≤‡∏∞‡πÉ‡∏ô inner loop ‡∏Ç‡∏≠‡∏á nested cross-validation ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏´‡∏≤ hyperparameters ‡∏ó‡∏µ‡πà‡∏î‡∏µ‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î
# * model_train: ‡∏ñ‡∏π‡∏Å‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÉ‡∏´‡∏°‡πà ‡πÇ‡∏î‡∏¢‡πÉ‡∏ä‡πâ best parameters ‡∏à‡∏≤‡∏Å model_cv ‡πÅ‡∏•‡πâ‡∏ß train ‡∏ö‡∏ô training set ‡∏ó‡∏±‡πâ‡∏á‡∏´‡∏°‡∏î ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡∏ú‡∏•‡∏ö‡∏ô training set
# * best_model: ‡πÄ‡∏Å‡πá‡∏ö‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ó‡∏µ‡πà‡∏î‡∏µ‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î

# **Metric ‡∏ó‡∏µ‡πà‡∏ô‡∏¥‡∏¢‡∏°‡πÉ‡∏ä‡πâ‡πÉ‡∏ô Regression:**
# 
# 1. **R-squared (R¬≤):**  
#       * ‡∏ß‡∏±‡∏î‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏±‡∏°‡∏û‡∏±‡∏ô‡∏ò‡πå‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á‡∏Ñ‡πà‡∏≤‡∏ó‡∏µ‡πà‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡πÅ‡∏•‡∏∞‡∏Ñ‡πà‡∏≤‡∏à‡∏£‡∏¥‡∏á  ‡∏Ñ‡πà‡∏≤ R¬≤ ‡πÉ‡∏Å‡∏•‡πâ 1  ‡∏ö‡πà‡∏á‡∏ä‡∏µ‡πâ‡∏ß‡πà‡∏≤‡πÅ‡∏ö‡∏ö‡∏à‡∏≥‡∏•‡∏≠‡∏á‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏û‡∏≠‡∏î‡∏µ‡∏Å‡∏±‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏î‡∏µ  ‡πÅ‡∏ï‡πà R¬≤  ‡∏≠‡∏≤‡∏à‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ç‡∏∂‡πâ‡∏ô‡πÑ‡∏î‡πâ  ‡πÅ‡∏°‡πâ‡∏ß‡πà‡∏≤‡∏à‡∏∞‡∏°‡∏µ‡∏Å‡∏≤‡∏£‡πÄ‡∏û‡∏¥‡πà‡∏°  features  ‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà  ‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á‡πÄ‡∏Ç‡πâ‡∏≤‡πÑ‡∏õ‡πÉ‡∏ô‡πÅ‡∏ö‡∏ö‡∏à‡∏≥‡∏•‡∏≠‡∏á 
# 2. **Mean Squared Error (MSE):**  
#     * ‡∏ß‡∏±‡∏î‡∏Ñ‡πà‡∏≤‡πÄ‡∏â‡∏•‡∏µ‡πà‡∏¢‡∏Ç‡∏≠‡∏á‡∏ú‡∏•‡∏ï‡πà‡∏≤‡∏á‡∏Å‡∏≥‡∏•‡∏±‡∏á‡∏™‡∏≠‡∏á‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á‡∏Ñ‡πà‡∏≤‡∏ó‡∏µ‡πà‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡πÅ‡∏•‡∏∞‡∏Ñ‡πà‡∏≤‡∏à‡∏£‡∏¥‡∏á  ‡∏¢‡∏¥‡πà‡∏á MSE ‡∏ï‡πà‡∏≥  ‡πÅ‡∏ö‡∏ö‡∏à‡∏≥‡∏•‡∏≠‡∏á‡∏¢‡∏¥‡πà‡∏á‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏°‡πà‡∏ô‡∏¢‡∏≥ 
# 3. **Root Mean Squared Error (RMSE):**  
#     * ‡πÄ‡∏õ‡πá‡∏ô‡∏£‡∏≤‡∏Å‡∏ó‡∏µ‡πà‡∏™‡∏≠‡∏á‡∏Ç‡∏≠‡∏á MSE  ‡∏ó‡∏≥‡πÉ‡∏´‡πâ  RMSE  ‡∏°‡∏µ‡∏´‡∏ô‡πà‡∏ß‡∏¢‡πÄ‡∏î‡∏µ‡∏¢‡∏ß‡∏Å‡∏±‡∏ö  target variable  ‡∏ã‡∏∂‡πà‡∏á‡∏ó‡∏≥‡πÉ‡∏´‡πâ‡∏ï‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡πÑ‡∏î‡πâ‡∏á‡πà‡∏≤‡∏¢‡∏Ç‡∏∂‡πâ‡∏ô 
# 4. **Mean Absolute Error (MAE):**  
#     * ‡∏ß‡∏±‡∏î‡∏Ñ‡πà‡∏≤‡πÄ‡∏â‡∏•‡∏µ‡πà‡∏¢‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Ñ‡∏•‡∏≤‡∏î‡πÄ‡∏Ñ‡∏•‡∏∑‡πà‡∏≠‡∏ô‡∏™‡∏±‡∏°‡∏ö‡∏π‡∏£‡∏ì‡πå‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á‡∏Ñ‡πà‡∏≤‡∏ó‡∏µ‡πà‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡πÅ‡∏•‡∏∞‡∏Ñ‡πà‡∏≤‡∏à‡∏£‡∏¥‡∏á  MAE  ‡πÉ‡∏´‡πâ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏Å‡∏±‡∏ö‡∏Ñ‡πà‡∏≤‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡∏ó‡∏∏‡∏Å‡∏Ñ‡πà‡∏≤‡πÄ‡∏ó‡πà‡∏≤‡πÜ ‡∏Å‡∏±‡∏ô  ‡πÑ‡∏°‡πà‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô  MSE  ‡∏ó‡∏µ‡πà‡πÉ‡∏´‡πâ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏Å‡∏±‡∏ö‡∏Ñ‡πà‡∏≤‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡∏ó‡∏µ‡πà‡∏™‡∏π‡∏á‡∏Å‡∏ß‡πà‡∏≤ 
# 5. **Mean Absolute Percentage Error (MAPE):**  
#     * ‡∏ß‡∏±‡∏î‡∏Ñ‡πà‡∏≤‡πÄ‡∏â‡∏•‡∏µ‡πà‡∏¢‡∏Ç‡∏≠‡∏á‡πÄ‡∏õ‡∏≠‡∏£‡πå‡πÄ‡∏ã‡πá‡∏ô‡∏ï‡πå‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Ñ‡∏•‡∏≤‡∏î‡πÄ‡∏Ñ‡∏•‡∏∑‡πà‡∏≠‡∏ô‡∏™‡∏±‡∏°‡∏ö‡∏π‡∏£‡∏ì‡πå‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á‡∏Ñ‡πà‡∏≤‡∏ó‡∏µ‡πà‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡πÅ‡∏•‡∏∞‡∏Ñ‡πà‡∏≤‡∏à‡∏£‡∏¥‡∏á  MAPE  ‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡πÄ‡∏õ‡πá‡∏ô‡πÄ‡∏õ‡∏≠‡∏£‡πå‡πÄ‡∏ã‡πá‡∏ô‡∏ï‡πå  ‡∏ó‡∏≥‡πÉ‡∏´‡πâ‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à‡∏á‡πà‡∏≤‡∏¢  ‡πÅ‡∏ï‡πà‡∏°‡∏µ‡∏õ‡∏±‡∏ç‡∏´‡∏≤‡πÄ‡∏°‡∏∑‡πà‡∏≠‡∏Ñ‡πà‡∏≤‡∏à‡∏£‡∏¥‡∏á‡πÄ‡∏õ‡πá‡∏ô‡∏®‡∏π‡∏ô‡∏¢‡πå 
# 
# **‡∏ß‡∏¥‡∏ò‡∏µ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å Metric:**
# 
# - **‡∏Ç‡∏∂‡πâ‡∏ô‡∏≠‡∏¢‡∏π‡πà‡∏Å‡∏±‡∏ö‡∏ß‡∏±‡∏ï‡∏ñ‡∏∏‡∏õ‡∏£‡∏∞‡∏™‡∏á‡∏Ñ‡πå‡∏Ç‡∏≠‡∏á‡πÅ‡∏ö‡∏ö‡∏à‡∏≥‡∏•‡∏≠‡∏á:**  
#     -  ‡∏ñ‡πâ‡∏≤‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡πÄ‡∏ô‡πâ‡∏ô‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏°‡πà‡∏ô‡∏¢‡∏≥‡πÇ‡∏î‡∏¢‡∏£‡∏ß‡∏°  ‡πÉ‡∏ä‡πâ  R¬≤ 
#     -  ‡∏ñ‡πâ‡∏≤‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏•‡∏î‡∏Ñ‡πà‡∏≤‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡∏ó‡∏µ‡πà‡∏™‡∏π‡∏á  ‡πÉ‡∏ä‡πâ  RMSE 
#     -  ‡∏ñ‡πâ‡∏≤‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡∏•‡∏î‡∏Ñ‡πà‡∏≤‡∏ú‡∏¥‡∏î‡∏û‡∏•‡∏≤‡∏î‡πÇ‡∏î‡∏¢‡πÄ‡∏â‡∏•‡∏µ‡πà‡∏¢  ‡πÉ‡∏ä‡πâ  MAE 
#     -  ‡∏ñ‡πâ‡∏≤‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ñ‡πà‡∏≤‡∏à‡∏£‡∏¥‡∏á‡πÄ‡∏õ‡πá‡∏ô‡∏®‡∏π‡∏ô‡∏¢‡πå  ‡πÅ‡∏•‡∏∞‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£ metric ‡∏ó‡∏µ‡πà‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à‡∏á‡πà‡∏≤‡∏¢  ‡πÉ‡∏ä‡πâ  MAPE 
# 
# 
# 

# ### 6.2.1 Multiple Linear Regression

# In[58]:


# --- Multiple Linear Regression ---
print("----- Multiple Linear Regression -----")

# ‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô (1) ‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏•
model_lr = LinearRegression()

# ‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô (2) ‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≥‡∏´‡∏ô‡∏î parameter grid 

# ‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô (3-4)  K-Fold Cross Validation (‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πà Nested)
cv = KFold(n_splits=5, shuffle=True, random_state=42)

scores_r2 = cross_val_score(model_lr, X_train, y_train, cv=cv, scoring='r2')
scores_mae = cross_val_score(model_lr, X_train, y_train, cv=cv, scoring='neg_mean_absolute_error')
scores_rmse = cross_val_score(model_lr, X_train, y_train, cv=cv, scoring='neg_root_mean_squared_error')
#scores_mse = cross_val_score(model_lr, X_train, y_train, cv=cv, scoring='neg_mean_squared_error') 

# ‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå cross-validation
print("Cross-Validation Results:")
print(f"  Average R-squared (R2): {np.mean(scores_r2):.4f}")
print(f"  Average Mean Absolute Error (MAE): {-np.mean(scores_mae):.4f}")
#print(f"  Average Mean Squared Error (MSE): {-np.mean(scores_mse):.4f}")
print(f"  Average Root Mean Squared Error (RMSE): {-np.mean(scores_rmse):.4f}")


# ‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô (5-6)  ‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡∏ú‡∏• Overfitting (‡πÑ‡∏°‡πà‡∏à‡∏≥‡πÄ‡∏õ‡πá‡∏ô‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö Linear Regression ‡πÄ‡∏û‡∏£‡∏≤‡∏∞ ‡πÑ‡∏°‡πà‡∏°‡∏µ parameter grid‡πÑ‡∏°‡πà‡∏ï‡πâ‡∏≠‡∏á‡πÉ‡∏ä‡πâ GridSearchCV )
# ... (Optional) 

# ‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô (7) Train Final Model
final_model_lr = model_lr
final_model_lr.fit(X_train, y_train)

# ‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô (8) ‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡∏ú‡∏• Final Model ‡∏ö‡∏ô test set
y_pred_lr = final_model_lr.predict(X_test)

# ‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô (9) ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì metrics
test_r2_lr = r2_score(y_test, y_pred_lr)
test_mae_lr = mean_absolute_error(y_test, y_pred_lr)
#test_mse_lr = mean_squared_error(y_test, y_pred_lr)
test_rmse_lr = mean_squared_error(y_test, y_pred_lr, squared=False)
test_mape_lr = mean_absolute_percentage_error(y_test, y_pred_lr)*100

# ‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå metrics
print("\nTest Set Results:")
print(f"  R-squared (Test Set) : {test_r2_lr:.4f}")
print(f"  MAE (Test Set) : {test_mae_lr:.4f}")
#print(f"  MSE (Test Set): {test_mse_lr:.4f}")
print(f"  RMSE (Test Set) : {test_rmse_lr:.4f}")
print(f"  MAPE (Test Set): {test_mape_lr:.4f}%\n")


# In[59]:


# --- ‡∏û‡∏•‡πá‡∏≠‡∏ï‡∏Å‡∏£‡∏≤‡∏ü Multiple Linear Regression - Residual Plot ---
plt.figure(figsize=(6, 4))
plt.scatter(y_pred_lr, y_test - y_pred_lr) 
plt.xlabel("Predicted Values")
plt.ylabel("Residuals")
plt.title("Multiple Linear Regression - Residual Plot")
plt.axhline(y=0, color='r', linestyle='--')
plt.show()


# ### 6.2.2 Ridge Regression

# In[60]:


# --- Ridge Regression ---
print("----- Ridge Regression -----")

# ‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô (1) ‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏•
model_ridge = Ridge(random_state=42)

# ‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô (2) ‡∏Å‡∏≥‡∏´‡∏ô‡∏î parameter grid 
param_grid_ridge = {'alpha': [0.1, 1, 10]}

# ‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô (3-4) Nested Cross-Validation
outer_cv = KFold(n_splits=5, shuffle=True, random_state=42)
inner_cv = KFold(n_splits=5, shuffle=True, random_state=42)

nested_scores_ridge = []
for train_index, test_index in outer_cv.split(X, y): 
    X_train_fold, X_test_fold = X[train_index], X[test_index]
    y_train_fold, y_test_fold = y[train_index], y[test_index]

    model_cv_ridge = GridSearchCV(model_ridge, param_grid_ridge, cv=inner_cv, scoring='r2')  
    model_cv_ridge.fit(X_train_fold, y_train_fold)

    y_pred_cv_ridge = model_cv_ridge.predict(X_test_fold)
    r2_ridge = r2_score(y_test_fold, y_pred_cv_ridge)
    nested_scores_ridge.append(r2_ridge)

# ‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô (5) ‡πÄ‡∏Å‡πá‡∏ö‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå nested cross-validation
results_ridge = {
    'cv_r2': np.mean(nested_scores_ridge),
    'best_params': model_cv_ridge.best_params_,
    'best_model': model_cv_ridge.best_estimator_
}

# ‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå nested cross-validation
print(f"Best Parameters: {results_ridge['best_params']}\n")
print("Nested Cross-Validation R2 scores:", nested_scores_ridge)
print(f"Average R2 (Validation Set): {results_ridge['cv_r2']:.4f}\n")

#  ‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô (6) ‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡∏ú‡∏• Overfitting
model_train_ridge = Ridge(**results_ridge['best_params'], random_state=42)
model_train_ridge.fit(X_train, y_train)

y_pred_train_ridge = model_train_ridge.predict(X_train)
train_r2_ridge = r2_score(y_train, y_pred_train_ridge)
print(f"R-squared (Training Set): {train_r2_ridge:.4f}")
print(f"R-squared diff: {train_r2_ridge - results_ridge['cv_r2']:.4f}\n")

#  ‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô (7)Train Final Model
final_model_ridge = results_ridge['best_model']
final_model_ridge.fit(X_train, y_train)

# ‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô (8) ‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡∏ú‡∏• Final Model ‡∏ö‡∏ô test set
y_pred_ridge = final_model_ridge.predict(X_test)

# ‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô (9) ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì metrics
test_r2_ridge = r2_score(y_test, y_pred_ridge)
test_mae_ridge = mean_absolute_error(y_test, y_pred_ridge)
test_rmse_ridge = mean_squared_error(y_test, y_pred_ridge, squared=False)
test_mape_ridge = mean_absolute_percentage_error(y_test, y_pred_ridge)*100

#  ‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå metrics
print("\nTest Set Results:")
print(f"  R-squared (Test Set): {test_r2_ridge:.4f}")
print(f"  MAE (Test Set): {test_mae_ridge:.4f}")
print(f"  RMSE (Test Set): {test_rmse_ridge:.4f}")
print(f"  MAPE (Test Set): {test_mape_ridge:.4f}%\n")


# In[61]:


# --- ‡∏û‡∏•‡πá‡∏≠‡∏ï‡∏Å‡∏£‡∏≤‡∏ü Ridge Regression - Residual Plot ---
plt.figure(figsize=(6,4 ))
plt.scatter(y_pred_ridge, y_test - y_pred_ridge)  # ‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡∏ï‡∏£‡∏á‡∏ô‡∏µ‡πâ
plt.xlabel("Predicted Values")
plt.ylabel("Residuals")
plt.title("Ridge Regression - Residual Plot")
plt.axhline(y=0, color='r', linestyle='--')
plt.show()


# ### 6.2.3 Lasso Regression

# * Lasso Regression ‡πÉ‡∏ä‡πâ L1 regularization ‡∏ã‡∏∂‡πà‡∏á‡∏°‡∏µ‡πÅ‡∏ô‡∏ß‡πÇ‡∏ô‡πâ‡∏°‡∏ó‡∏≥‡πÉ‡∏´‡πâ coefficients ‡∏ö‡∏≤‡∏á‡∏ï‡∏±‡∏ß‡πÄ‡∏õ‡πá‡∏ô‡∏®‡∏π‡∏ô‡∏¢‡πå ‡∏ä‡πà‡∏ß‡∏¢‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡πÄ‡∏•‡∏∑‡∏≠‡∏Å feature

# In[62]:


# --- Lasso Regression ---
print("----- Lasso Regression -----")

# ‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô (1) ‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏•
model_lasso = Lasso(random_state=42)

# ‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô (2) ‡∏Å‡∏≥‡∏´‡∏ô‡∏î parameter grid 
param_grid_lasso = {'alpha': [0.1, 1, 10]}

# ‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô (3-4) Nested Cross-Validation
outer_cv = KFold(n_splits=5, shuffle=True, random_state=42)
inner_cv = KFold(n_splits=5, shuffle=True, random_state=42)

nested_scores_lasso = []
for train_index, test_index in outer_cv.split(X, y): 
    X_train_fold, X_test_fold = X[train_index], X[test_index]
    y_train_fold, y_test_fold = y[train_index], y[test_index]

    model_cv_lasso = GridSearchCV(model_lasso, param_grid_lasso, cv=inner_cv, scoring='r2')  
    model_cv_lasso.fit(X_train_fold, y_train_fold)

    y_pred_cv_lasso = model_cv_lasso.predict(X_test_fold)
    r2_lasso = r2_score(y_test_fold, y_pred_cv_lasso)
    nested_scores_lasso.append(r2_lasso)

# ‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô (5) ‡πÄ‡∏Å‡πá‡∏ö‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå nested cross-validation
results_lasso = {
    'cv_r2': np.mean(nested_scores_lasso),
    'best_params': model_cv_lasso.best_params_,
    'best_model': model_cv_lasso.best_estimator_
}

# ‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå nested cross-validation
print(f"Best Parameters: {results_lasso['best_params']}\n")
print("Nested Cross-Validation R2 scores:", nested_scores_lasso)
print(f"Average R2 (Validation Set): {results_lasso['cv_r2']:.4f}\n")

#  ‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô (6) ‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡∏ú‡∏• Overfitting
model_train_lasso = Lasso(**results_lasso['best_params'], random_state=42)
model_train_lasso.fit(X_train, y_train)

y_pred_train_lasso = model_train_lasso.predict(X_train)
train_r2_lasso = r2_score(y_train, y_pred_train_lasso)
print(f"R-squared (Training Set): {train_r2_lasso:.4f}")
print(f"R-squared diff: {train_r2_lasso - results_lasso['cv_r2']:.4f}\n")

#  ‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô (7)Train Final Model
final_model_lasso = results_lasso['best_model']
final_model_lasso.fit(X_train, y_train)

# ‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô (8) ‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡∏ú‡∏• Final Model ‡∏ö‡∏ô test set
y_pred_lasso = final_model_lasso.predict(X_test)

# ‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô (9) ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì metrics
test_r2_lasso = r2_score(y_test, y_pred_lasso)
test_mae_lasso = mean_absolute_error(y_test, y_pred_lasso)
test_rmse_lasso = mean_squared_error(y_test, y_pred_lasso, squared=False)
test_mape_lasso = mean_absolute_percentage_error(y_test, y_pred_lasso)*100

#  ‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå metrics
print("\nTest Set Results:")
print(f"  R-squared (Test Set): {test_r2_lasso:.4f}")
print(f"  MAE (Test Set): {test_mae_lasso:.4f}")
print(f"  RMSE (Test Set): {test_rmse_lasso:.4f}")
print(f"  MAPE (Test Set): {test_mape_lasso:.4f}%\n")


# In[63]:


# --- ‡∏û‡∏•‡πá‡∏≠‡∏ï‡∏Å‡∏£‡∏≤‡∏ü Lasso Regression - Residual Plot ---
plt.figure(figsize=(6,4 ))
plt.scatter(y_pred_lasso, y_test - y_pred_lasso)  # ‡πÅ‡∏Å‡πâ‡πÑ‡∏Ç‡∏ï‡∏£‡∏á‡∏ô‡∏µ‡πâ
plt.xlabel("Predicted Values")
plt.ylabel("Residuals")
plt.title("Lasso Regression - Residual Plot")
plt.axhline(y=0, color='r', linestyle='--')
plt.show()


# ### 6.2.4 Elastic Net

# **‡∏û‡∏≤‡∏£‡∏≤‡∏°‡∏¥‡πÄ‡∏ï‡∏≠‡∏£‡πå‡∏Ç‡∏≠‡∏á ElasticNet**
# 
# `l1_ratio`  ‡πÉ‡∏ô ElasticNet ‡πÄ‡∏õ‡πá‡∏ô hyperparameter ‡∏ó‡∏µ‡πà‡∏Ñ‡∏ß‡∏ö‡∏Ñ‡∏∏‡∏°‡∏™‡∏±‡∏î‡∏™‡πà‡∏ß‡∏ô‡∏Ç‡∏≠‡∏á L1 regularization (Lasso) ‡πÅ‡∏•‡∏∞ L2 regularization (Ridge) ‡πÉ‡∏ô‡πÇ‡∏°‡πÄ‡∏î‡∏• 
# 
# **‡∏Ñ‡πà‡∏≤‡∏ó‡∏µ‡πà‡πÄ‡∏õ‡πá‡∏ô‡πÑ‡∏õ‡πÑ‡∏î‡πâ:**
# 
# * **0:** ‡πÉ‡∏ä‡πâ L2 regularization ‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô  ‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡∏Å‡∏±‡∏ö Ridge regression
# * **1:** ‡πÉ‡∏ä‡πâ L1 regularization ‡πÄ‡∏ó‡πà‡∏≤‡∏ô‡∏±‡πâ‡∏ô  ‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡∏Å‡∏±‡∏ö Lasso regression
# * **‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á 0 ‡∏ñ‡∏∂‡∏á 1:**  ‡πÉ‡∏ä‡πâ‡∏ó‡∏±‡πâ‡∏á L1 ‡πÅ‡∏•‡∏∞ L2 regularization   ‡πÇ‡∏î‡∏¢‡∏Ñ‡πà‡∏≤  `l1_ratio`  ‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡∏™‡∏±‡∏î‡∏™‡πà‡∏ß‡∏ô  ‡πÄ‡∏ä‡πà‡∏ô  `l1_ratio=0.5`  ‡∏´‡∏°‡∏≤‡∏¢‡∏ñ‡∏∂‡∏á‡πÉ‡∏ä‡πâ L1 50%  ‡πÅ‡∏•‡∏∞ L2 50% 
# 
# **‡∏ú‡∏•‡∏Å‡∏£‡∏∞‡∏ó‡∏ö‡∏Ç‡∏≠‡∏á l1_ratio:**
# 
# * **l1_ratio ‡πÉ‡∏Å‡∏•‡πâ 0:**  ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏°‡∏µ‡πÅ‡∏ô‡∏ß‡πÇ‡∏ô‡πâ‡∏°‡∏Ñ‡∏•‡πâ‡∏≤‡∏¢ Ridge  ‡∏•‡∏î‡∏ô‡πâ‡∏≥‡∏´‡∏ô‡∏±‡∏Å‡∏Ç‡∏≠‡∏á coefficients  ‡πÅ‡∏ï‡πà‡πÑ‡∏°‡πà‡∏ó‡∏≥‡πÉ‡∏´‡πâ‡πÄ‡∏õ‡πá‡∏ô  0 
# * **l1_ratio ‡πÉ‡∏Å‡∏•‡πâ 1:**  ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏°‡∏µ‡πÅ‡∏ô‡∏ß‡πÇ‡∏ô‡πâ‡∏°‡∏Ñ‡∏•‡πâ‡∏≤‡∏¢ Lasso  ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å features  ‡∏ó‡∏µ‡πà‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç  ‡πÅ‡∏•‡∏∞‡∏ó‡∏≥‡πÉ‡∏´‡πâ coefficients  ‡∏Ç‡∏≠‡∏á  features  ‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡πÄ‡∏õ‡πá‡∏ô  0  (feature selection)
# * **l1_ratio ‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á 0 ‡∏ñ‡∏∂‡∏á 1:**  ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ú‡∏™‡∏°‡∏ú‡∏™‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏î‡∏µ‡∏Ç‡∏≠‡∏á Ridge ‡πÅ‡∏•‡∏∞ Lasso  

# In[64]:


# --- ElasticNet Regression ---
print("----- ElasticNet Regression -----")

# ‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô (1) ‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏•
model_elasticnet = ElasticNet(random_state=42)

# ‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô (2) ‡∏Å‡∏≥‡∏´‡∏ô‡∏î parameter grid 
param_grid_elasticnet = {
    'alpha': [0.1, 1, 10],
    'l1_ratio': [0.1, 0.5, 0.9]  # ‡πÄ‡∏û‡∏¥‡πà‡∏° l1_ratio ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö ElasticNet
}

# ‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô (3-4) Nested Cross-Validation
outer_cv = KFold(n_splits=5, shuffle=True, random_state=42)
inner_cv = KFold(n_splits=5, shuffle=True, random_state=42)

nested_scores_elasticnet = []
for train_index, test_index in outer_cv.split(X, y): 
    X_train_fold, X_test_fold = X[train_index], X[test_index]
    y_train_fold, y_test_fold = y[train_index], y[test_index]

    model_cv_elasticnet = GridSearchCV(model_elasticnet, param_grid_elasticnet, cv=inner_cv, scoring='r2')  
    model_cv_elasticnet.fit(X_train_fold, y_train_fold)

    y_pred_cv_elasticnet = model_cv_elasticnet.predict(X_test_fold)
    r2_elasticnet = r2_score(y_test_fold, y_pred_cv_elasticnet)
    nested_scores_elasticnet.append(r2_elasticnet)

# ‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô (5) ‡πÄ‡∏Å‡πá‡∏ö‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå nested cross-validation
results_elasticnet = {
    'cv_r2': np.mean(nested_scores_elasticnet),
    'best_params': model_cv_elasticnet.best_params_,
    'best_model': model_cv_elasticnet.best_estimator_
}

# ‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå nested cross-validation
print(f"Best Parameters: {results_elasticnet['best_params']}\n")
print("Nested Cross-Validation R2 scores:", nested_scores_elasticnet)
print(f"Average R2 (Validation Set): {results_elasticnet['cv_r2']:.4f}\n")

#  ‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô (6) ‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡∏ú‡∏• Overfitting
model_train_elasticnet = ElasticNet(**results_elasticnet['best_params'], random_state=42)
model_train_elasticnet.fit(X_train, y_train)

y_pred_train_elasticnet = model_train_elasticnet.predict(X_train)
train_r2_elasticnet = r2_score(y_train, y_pred_train_elasticnet)
print(f"R-squared (Training Set): {train_r2_elasticnet:.4f}")
print(f"R-squared diff: {train_r2_elasticnet - results_elasticnet['cv_r2']:.4f}\n")

#  ‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô (7) Train Final Model
final_model_elasticnet = results_elasticnet['best_model']
final_model_elasticnet.fit(X_train, y_train)

# ‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô (8) ‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡∏ú‡∏• Final Model ‡∏ö‡∏ô test set
y_pred_elasticnet = final_model_elasticnet.predict(X_test)

# ‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô (9) ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì metrics
test_r2_elasticnet = r2_score(y_test, y_pred_elasticnet)
test_mae_elasticnet = mean_absolute_error(y_test, y_pred_elasticnet)
test_rmse_elasticnet = mean_squared_error(y_test, y_pred_elasticnet, squared=False)
test_mape_elasticnet = mean_absolute_percentage_error(y_test, y_pred_elasticnet)*100

#  ‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå metrics
print("\nTest Set Results:")
print(f"  R-squared (Test Set): {test_r2_elasticnet:.4f}")
print(f"  MAE (Test Set): {test_mae_elasticnet:.4f}")
print(f"  RMSE (Test Set): {test_rmse_elasticnet:.4f}")
print(f"  MAPE (Test Set): {test_mape_elasticnet:.4f}%\n")


# In[65]:


# --- ‡∏û‡∏•‡πá‡∏≠‡∏ï‡∏Å‡∏£‡∏≤‡∏ü Residual Plot - ElasticNet Regression --- 
# ‡πÅ‡∏™‡∏î‡∏á‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏±‡∏°‡∏û‡∏±‡∏ô‡∏ò‡πå‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á‡∏Ñ‡πà‡∏≤‡∏ó‡∏µ‡πà‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡πÅ‡∏•‡∏∞ residuals

plt.figure(figsize=(6, 4))
plt.scatter(y_pred_elasticnet, y_test - y_pred_elasticnet)
plt.xlabel("Predicted Values")
plt.ylabel("Residuals")
plt.title("ElasticNet Regression - Residual Plot")
plt.axhline(y=0, color='r', linestyle='--')
plt.show()


# ####  ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏´‡∏°‡∏≤‡∏¢‡∏Ç‡∏≠‡∏á‡∏Å‡∏£‡∏≤‡∏ü Residual Plot

# * ‡∏Å‡∏£‡∏≤‡∏ü Residual Plot ‡πÅ‡∏™‡∏î‡∏á‡πÉ‡∏´‡πâ‡πÄ‡∏´‡πá‡∏ô‡∏ß‡πà‡∏≤ Linear Model ‡∏≠‡∏≤‡∏à‡∏à‡∏∞ **‡πÑ‡∏°‡πà‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏°** ‡∏Å‡∏±‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•  
# * ‡πÅ‡∏•‡∏∞‡∏¢‡∏±‡∏á‡∏ä‡∏µ‡πâ‡πÉ‡∏´‡πâ‡πÄ‡∏´‡πá‡∏ô‡∏ß‡πà‡∏≤ **‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏±‡∏°‡∏û‡∏±‡∏ô‡∏ò‡πå‡πÅ‡∏ö‡∏ö non-linear** ‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á features ‡πÅ‡∏•‡∏∞ target variable  
# * **‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏™‡∏∏‡πà‡∏°‡∏Ç‡∏≠‡∏á residuals:**   Residuals  ‡πÑ‡∏°‡πà‡πÑ‡∏î‡πâ‡∏Å‡∏£‡∏∞‡∏à‡∏≤‡∏¢‡πÅ‡∏ö‡∏ö‡∏™‡∏∏‡πà‡∏°‡∏£‡∏≠‡∏ö‡πÄ‡∏™‡πâ‡∏ô‡πÅ‡∏ô‡∏ß‡∏ô‡∏≠‡∏ô‡∏ó‡∏µ‡πà  0 (‡πÄ‡∏™‡πâ‡∏ô‡∏™‡∏µ‡πÅ‡∏î‡∏á)   ‡πÅ‡∏ï‡πà‡∏Å‡∏•‡∏±‡∏ö‡∏°‡∏µ **‡πÅ‡∏ô‡∏ß‡πÇ‡∏ô‡πâ‡∏°‡∏•‡∏î‡∏•‡∏á**   ‡πÄ‡∏°‡∏∑‡πà‡∏≠  predicted values  ‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ç‡∏∂‡πâ‡∏ô  ‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡∏ô‡∏µ‡πâ‡∏ö‡πà‡∏á‡∏ä‡∏µ‡πâ‡∏ß‡πà‡∏≤ linear model ‡πÑ‡∏°‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏à‡∏±‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏±‡∏°‡∏û‡∏±‡∏ô‡∏ò‡πå‡πÉ‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡πÑ‡∏î‡πâ‡∏î‡∏µ   
# *  ‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏õ‡∏£‡∏õ‡∏£‡∏ß‡∏ô‡∏Ç‡∏≠‡∏á  residuals  ‡πÑ‡∏°‡πà‡∏Ñ‡∏á‡∏ó‡∏µ‡πà  ‡πÄ‡∏û‡∏£‡∏≤‡∏∞‡∏Ñ‡πà‡∏≤ Residuals  ‡∏°‡∏µ‡∏Å‡∏≤‡∏£‡∏Å‡∏£‡∏∞‡∏à‡∏≤‡∏¢‡∏ï‡∏±‡∏ß‡∏Å‡∏ß‡πâ‡∏≤‡∏á  ‡πÄ‡∏°‡∏∑‡πà‡∏≠  predicted values  ‡∏ï‡πà‡∏≥  ‡πÅ‡∏•‡∏∞‡πÅ‡∏Ñ‡∏ö‡∏•‡∏á  ‡πÄ‡∏°‡∏∑‡πà‡∏≠  predicted values  ‡∏™‡∏π‡∏á‡∏Ç‡∏∂‡πâ‡∏ô  
#     ‡∏ö‡πà‡∏á‡∏ä‡∏µ‡πâ‡∏ß‡πà‡∏≤  linear model  ‡∏≠‡∏≤‡∏à‡∏à‡∏∞‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢  claim amount  ‡πÑ‡∏î‡πâ‡πÑ‡∏°‡πà‡πÅ‡∏°‡πà‡∏ô‡∏¢‡∏≥‡πÄ‡∏ó‡πà‡∏≤‡∏Å‡∏±‡∏ô‡πÉ‡∏ô‡∏ó‡∏∏‡∏Å‡∏ä‡πà‡∏ß‡∏á‡∏Ñ‡πà‡∏≤  
# 

# ### üß°üíñ ‡πÄ‡∏°‡∏∑‡πà‡∏≠‡∏û‡∏¥‡∏à‡∏≤‡∏ì‡∏≤‡∏à‡∏≤‡∏Å R-squared ‡∏ö‡∏ô test set  , ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏±‡∏°‡∏û‡∏±‡∏ô‡∏ò‡πå‡∏Ç‡∏≠‡∏á‡∏ï‡∏±‡∏ß‡πÅ‡∏õ‡∏£ (Correlation), ‡∏Å‡∏£‡∏≤‡∏ü Pairplot ‡πÅ‡∏•‡∏∞ ‡∏Å‡∏£‡∏≤‡∏ü Residual Plot  
# * ‡πÅ‡∏™‡∏î‡∏á‡πÉ‡∏´‡πâ‡πÄ‡∏´‡πá‡∏ô‡∏ß‡πà‡∏≤ linear model ‡∏≠‡∏≤‡∏à‡∏à‡∏∞‡πÑ‡∏°‡πà‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏°‡∏Å‡∏±‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•  
# * ‡πÅ‡∏•‡∏∞‡∏ö‡πà‡∏á‡∏ä‡∏µ‡πâ‡∏ß‡πà‡∏≤‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏±‡∏°‡∏û‡∏±‡∏ô‡∏ò‡πå‡πÅ‡∏ö‡∏ö  non-linear  ‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡πÉ‡∏ä‡πâ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ó‡∏µ‡πà‡∏ã‡∏±‡∏ö‡∏ã‡πâ‡∏≠‡∏ô‡∏°‡∏≤‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡∏õ‡∏£‡∏∞‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡∏†‡∏≤‡∏û‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢
# * ‡πÇ‡∏î‡∏¢‡∏à‡∏∞‡πÉ‡∏ä‡πâ Decision tree l, Random forest model ‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢

# ### 6.2.5 Decision Tree Regression

# In[66]:


# --- Decision Tree Regression ---
print("----- Decision Tree Regression -----")

# ‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô (1) ‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏•
model_dt = DecisionTreeRegressor(random_state=2)

# ‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô (2) ‡∏Å‡∏≥‡∏´‡∏ô‡∏î parameter grid 
param_grid_dt = {
    'max_depth': [7, 10, 12],
    'min_samples_split': [30,35,40],
    'min_samples_leaf': [ 4, 7, 9]
}

# ‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô (3-4) Nested Cross-Validation
outer_cv = KFold(n_splits=5, shuffle=True, random_state=2)
inner_cv = KFold(n_splits=5, shuffle=True, random_state=2)

nested_scores_dt = []
for train_index, test_index in outer_cv.split(X, y): 
    X_train_fold, X_test_fold = X[train_index], X[test_index]
    y_train_fold, y_test_fold = y[train_index], y[test_index]

    model_cv_dt = GridSearchCV(model_dt, param_grid_dt, cv=inner_cv, scoring='r2')  
    model_cv_dt.fit(X_train_fold, y_train_fold)

    y_pred_cv_dt = model_cv_dt.predict(X_test_fold)
    r2_dt = r2_score(y_test_fold, y_pred_cv_dt)
    nested_scores_dt.append(r2_dt)

# ‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô (5) ‡πÄ‡∏Å‡πá‡∏ö‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå nested cross-validation
results_dt = {
    'cv_r2': np.mean(nested_scores_dt),
    'best_params': model_cv_dt.best_params_,
    'best_model': model_cv_dt.best_estimator_
}

# ‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå nested cross-validation
print(f"Best Parameters: {results_dt['best_params']}\n")
print("Nested Cross-Validation R2 scores:", nested_scores_dt)
print(f"Average R2 (Validation Set): {results_dt['cv_r2']:.4f}\n")

#  ‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô (6) ‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡∏ú‡∏• Overfitting
model_train_dt = DecisionTreeRegressor(**results_dt['best_params'], random_state=2)
model_train_dt.fit(X_train, y_train)

y_pred_train_dt = model_train_dt.predict(X_train)
train_r2_dt = r2_score(y_train, y_pred_train_dt)
print(f"R-squared (Training Set): {train_r2_dt:.4f}")
print(f"R-squared diff: {train_r2_dt - results_dt['cv_r2']:.4f}\n")

#  ‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô (7) Train Final Model
final_model_dt = results_dt['best_model']
final_model_dt.fit(X_train, y_train)

# ‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô (8) ‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡∏ú‡∏• Final Model ‡∏ö‡∏ô test set
y_pred_dt = final_model_dt.predict(X_test)

# ‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô (9) ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì metrics
test_r2_dt = r2_score(y_test, y_pred_dt)
test_mae_dt = mean_absolute_error(y_test, y_pred_dt)
test_rmse_dt = mean_squared_error(y_test, y_pred_dt, squared=False)
test_mape_dt = mean_absolute_percentage_error(y_test, y_pred_dt)*100

#  ‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå metrics
print("\nTest Set Results:")
print(f"  R-squared (Test Set): {test_r2_dt:.4f}")
print(f"  MAE (Test Set): {test_mae_dt:.4f}")
print(f"  RMSE (Test Set): {test_rmse_dt:.4f}")
print(f"  MAPE (Test Set): {test_mape_dt:.4f}%\n")


# ### 6.2.6 Random Forest Regression

# In[67]:


# --- Random Forest Regression ---
print("----- Random Forest Regression -----")

# ‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô (1) ‡∏Å‡∏≥‡∏´‡∏ô‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏•
model_rf = RandomForestRegressor(random_state=2)

# ‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô (2) ‡∏Å‡∏≥‡∏´‡∏ô‡∏î parameter grid 
param_grid_rf = {
    'n_estimators': [100,150, 200], 
    'max_depth': [7, 10, 12],
    'min_samples_split': [25, 30,35],
    'min_samples_leaf': [4, 5, 7]
}

# ‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô (3-4) Nested Cross-Validation
outer_cv = KFold(n_splits=5, shuffle=True, random_state=2)
inner_cv = KFold(n_splits=5, shuffle=True, random_state=2)

nested_scores_rf = []
for train_index, test_index in outer_cv.split(X, y): 
    X_train_fold, X_test_fold = X[train_index], X[test_index]
    y_train_fold, y_test_fold = y[train_index], y[test_index]

    model_cv_rf = GridSearchCV(model_rf, param_grid_rf, cv=inner_cv, scoring='r2')  
    model_cv_rf.fit(X_train_fold, y_train_fold)

    y_pred_cv_rf = model_cv_rf.predict(X_test_fold)
    r2_rf = r2_score(y_test_fold, y_pred_cv_rf)
    nested_scores_rf.append(r2_rf)

# ‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô (5) ‡πÄ‡∏Å‡πá‡∏ö‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå nested cross-validation
results_rf = {
    'cv_r2': np.mean(nested_scores_rf),
    'best_params': model_cv_rf.best_params_,
    'best_model': model_cv_rf.best_estimator_
}

# ‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå nested cross-validation
print(f"Best Parameters: {results_rf['best_params']}\n")
print("Nested Cross-Validation R2 scores:", nested_scores_rf)
print(f"Average R2 (Validation Set): {results_rf['cv_r2']:.4f}\n")

#  ‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô (6) ‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡∏ú‡∏• Overfitting
model_train_rf = RandomForestRegressor(**results_rf['best_params'], random_state=2)
model_train_rf.fit(X_train, y_train)

y_pred_train_rf = model_train_rf.predict(X_train)
train_r2_rf = r2_score(y_train, y_pred_train_rf)
print(f"R-squared (Training Set): {train_r2_rf:.4f}")
print(f"R-squared diff: {train_r2_rf - results_rf['cv_r2']:.4f}\n")

#  ‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô (7) Train Final Model
final_model_rf = results_rf['best_model']
final_model_rf.fit(X_train, y_train)

# ‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô (8) ‡∏õ‡∏£‡∏∞‡πÄ‡∏°‡∏¥‡∏ô‡∏ú‡∏• Final Model ‡∏ö‡∏ô test set
y_pred_rf = final_model_rf.predict(X_test)

# ‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô (9) ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì metrics
test_r2_rf = r2_score(y_test, y_pred_rf)
test_mae_rf = mean_absolute_error(y_test, y_pred_rf)
test_rmse_rf = mean_squared_error(y_test, y_pred_rf, squared=False)
test_mape_rf = mean_absolute_percentage_error(y_test, y_pred_rf)*100

#  ‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå metrics
print("\nTest Set Results:")
print(f"  R-squared (Test Set): {test_r2_rf:.4f}")
print(f"  MAE (Test Set): {test_mae_rf:.4f}")
print(f"  RMSE (Test Set): {test_rmse_rf:.4f}")
print(f"  MAPE (Test Set): {test_mape_rf:.4f}%\n")


# ## 6.3 ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å Best Model

# ### 6.3.1 ‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå metrics ‡∏Ç‡∏≠‡∏á‡πÅ‡∏ï‡πà‡∏•‡∏∞‡πÇ‡∏°‡πÄ‡∏î‡∏•

# In[68]:


# ‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå metrics  ---  Multiple Linear Regression  ---
print("----- Multiple Linear Regression -----")
print("\nTest Set Results:")
print(f"  R-squared (Test Set) : {test_r2_lr:.4f}")
print(f"  MAE (Test Set) : {test_mae_lr:.4f}")
print(f"  RMSE (Test Set) : {test_rmse_lr:.4f}")
print(f"  MAPE (Test Set): {test_mape_lr:.4f}%\n")

# ‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå metrics  ---   Ridge Regression   ---
print("----- Ridge Regression -----")
print("\nTest Set Results:")
print(f"  R-squared (Test Set): {test_r2_ridge:.4f}")
print(f"  MAE (Test Set): {test_mae_ridge:.4f}")
print(f"  RMSE (Test Set): {test_rmse_ridge:.4f}")
print(f"  MAPE (Test Set): {test_mape_ridge:.4f}%\n")

# ‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå metrics  ---   Lasso Regression   ---
print("----- Lasso Regression -----")
print("\nTest Set Results:")
print(f"  R-squared (Test Set): {test_r2_lasso:.4f}")
print(f"  MAE (Test Set): {test_mae_lasso:.4f}")
print(f"  RMSE (Test Set): {test_rmse_lasso:.4f}")
print(f"  MAPE (Test Set): {test_mape_lasso:.4f}%\n")

# ‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå metrics  ---   ElasticNet Regression   ---
print("----- ElasticNet Regression -----")
print("\nTest Set Results:")
print(f"  R-squared (Test Set): {test_r2_elasticnet:.4f}")
print(f"  MAE (Test Set): {test_mae_elasticnet:.4f}")
print(f"  RMSE (Test Set): {test_rmse_elasticnet:.4f}")
print(f"  MAPE (Test Set): {test_mape_elasticnet:.4f}%\n")

# ‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå metrics  ---   Decision Tree  ---
print("----- Decision Tree Regression -----")
print("\nTest Set Results:")
print(f"  R-squared (Test Set): {test_r2_dt:.4f}")
print(f"  MAE (Test Set): {test_mae_dt:.4f}")
print(f"  RMSE (Test Set): {test_rmse_dt:.4f}")
print(f"  MAPE (Test Set): {test_mape_dt:.4f}%\n")

# ‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå metrics  ---  Random Forest Regression  ---
print("----- Random Forest Regression -----")
print("\nTest Set Results:")
print(f"  R-squared (Test Set): {test_r2_rf:.4f}")
print(f"  MAE (Test Set): {test_mae_rf:.4f}")
print(f"  RMSE (Test Set): {test_rmse_rf:.4f}")
print(f"  MAPE (Test Set): {test_mape_rf:.4f}%\n")


# ### 6.3.2 ‡πÅ‡∏™‡∏î‡∏á‡∏Å‡∏£‡∏≤‡∏ü‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö‡∏Ñ‡πà‡∏≤ MAPE ‡∏Ç‡∏≠‡∏á‡πÅ‡∏ï‡πà‡∏•‡∏∞‡πÇ‡∏°‡πÄ‡∏î‡∏•

# **Mean Absolute Percentage Error (MAPE):**
# * ‡∏ß‡∏±‡∏î‡∏Ñ‡πà‡∏≤‡πÄ‡∏â‡∏•‡∏µ‡πà‡∏¢‡∏Ç‡∏≠‡∏á‡πÄ‡∏õ‡∏≠‡∏£‡πå‡πÄ‡∏ã‡πá‡∏ô‡∏ï‡πå‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Ñ‡∏•‡∏≤‡∏î‡πÄ‡∏Ñ‡∏•‡∏∑‡πà‡∏≠‡∏ô‡∏™‡∏±‡∏°‡∏ö‡∏π‡∏£‡∏ì‡πå‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á‡∏Ñ‡πà‡∏≤‡∏ó‡∏µ‡πà‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡πÅ‡∏•‡∏∞‡∏Ñ‡πà‡∏≤‡∏à‡∏£‡∏¥‡∏á  
# * MAPE  ‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡πÄ‡∏õ‡πá‡∏ô‡πÄ‡∏õ‡∏≠‡∏£‡πå‡πÄ‡∏ã‡πá‡∏ô‡∏ï‡πå  ‡∏ó‡∏≥‡πÉ‡∏´‡πâ‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à‡∏á‡πà‡∏≤‡∏¢
# * ‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏• MAPE ‡πÉ‡∏ô‡∏£‡∏π‡∏õ‡πÅ‡∏ö‡∏ö‡∏Å‡∏£‡∏≤‡∏ü‡πÅ‡∏ó‡πà‡∏á (bar chart) ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö MAPE ‡∏Ç‡∏≠‡∏á‡πÅ‡∏ï‡πà‡∏•‡∏∞‡πÇ‡∏°‡πÄ‡∏î‡∏•

# In[69]:


models = ['Multiple Linear Regression', 'Ridge Regression', 'Lasso Regression', 
                'ElasticNet Regression', 'Decision Tree Regression', 'Random Forest Regression']
mape_values = [15.1591, 15.1720, 15.0804, 15.2511, 6.0355, 5.6379]

plt.figure(figsize=(8, 6))  
plt.barh(models, mape_values, color='y' )  
plt.xlabel('MAPE (%)')        
#plt.ylabel('Model')
plt.title('Comparison of MAPE for Different Models')
plt.show()


# ‡∏Å‡∏£‡∏≤‡∏ü‡∏ô‡∏µ‡πâ‡πÅ‡∏™‡∏î‡∏á‡∏Å‡∏≤‡∏£‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö‡∏Ñ‡πà‡∏≤ MAPE (Mean Absolute Percentage Error) ‡∏Ç‡∏≠‡∏á‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡πÅ‡∏ö‡∏ö Regression ‡∏ó‡∏µ‡πà‡πÅ‡∏ï‡∏Å‡∏ï‡πà‡∏≤‡∏á‡∏Å‡∏±‡∏ô 6 ‡πÇ‡∏°‡πÄ‡∏î‡∏• ‡πÑ‡∏î‡πâ‡πÅ‡∏Å‡πà:
# 
# 1. Multiple Linear Regression
# 2. Ridge Regression
# 3. Lasso Regression
# 4. ElasticNet Regression
# 5. Decision Tree Regression
# 6. Random Forest Regression
# 
# ‡∏à‡∏≤‡∏Å‡∏Å‡∏£‡∏≤‡∏ü ‡∏à‡∏∞‡πÄ‡∏´‡πá‡∏ô‡πÑ‡∏î‡πâ‡∏ß‡πà‡∏≤ **Random Forest Regression** ‡∏°‡∏µ‡∏Ñ‡πà‡∏≤ MAPE ‡∏ô‡πâ‡∏≠‡∏¢‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î (‡∏õ‡∏£‡∏∞‡∏°‡∏≤‡∏ì 5.64%)  ‡∏£‡∏≠‡∏á‡∏•‡∏á‡∏°‡∏≤‡∏Ñ‡∏∑‡∏≠ Decision Tree Regression (‡∏õ‡∏£‡∏∞‡∏°‡∏≤‡∏ì 6.04%)  
# 
# ‡∏™‡πà‡∏ß‡∏ô‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ó‡∏µ‡πà‡πÄ‡∏´‡∏•‡∏∑‡∏≠ (Multiple Linear Regression, Ridge Regression, Lasso Regression, ElasticNet Regression) ‡∏°‡∏µ‡∏Ñ‡πà‡∏≤ MAPE ‡πÉ‡∏Å‡∏•‡πâ‡πÄ‡∏Ñ‡∏µ‡∏¢‡∏á‡∏Å‡∏±‡∏ô  ‡∏≠‡∏¢‡∏π‡πà‡∏ó‡∏µ‡πà‡∏õ‡∏£‡∏∞‡∏°‡∏≤‡∏ì 15%  ‡∏ã‡∏∂‡πà‡∏á‡∏™‡∏π‡∏á‡∏Å‡∏ß‡πà‡∏≤  Random Forest  ‡πÅ‡∏•‡∏∞  Decision Tree  ‡∏°‡∏≤‡∏Å
# 
# **‚úÖ‡∏™‡∏£‡∏∏‡∏õ:**
# 
# ‡πÇ‡∏°‡πÄ‡∏î‡∏• **Random Forest Regression** ‡πÄ‡∏´‡∏°‡∏≤‡∏∞‡∏™‡∏°‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡πÉ‡∏ô‡∏Å‡∏£‡∏ì‡∏µ‡∏ô‡∏µ‡πâ  ‡πÄ‡∏ô‡∏∑‡πà‡∏≠‡∏á‡∏à‡∏≤‡∏Å‡∏°‡∏µ‡∏Ñ‡πà‡∏≤ MAPE ‡∏ô‡πâ‡∏≠‡∏¢‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î  ‡∏´‡∏°‡∏≤‡∏¢‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ß‡πà‡∏≤‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏°‡πà‡∏ô‡∏¢‡∏≥‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏™‡∏π‡∏á‡∏Å‡∏ß‡πà‡∏≤‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏≠‡∏∑‡πà‡∏ô‡πÜ  
# 
# 
# 

# ### 6.3.3 Feature Importance for Random Forest 

# In[82]:


# ‡πÄ‡∏£‡∏µ‡∏¢‡∏á‡∏•‡∏≥‡∏î‡∏±‡∏ö feature importance ‡∏à‡∏≤‡∏Å‡∏°‡∏≤‡∏Å‡πÑ‡∏õ‡∏ô‡πâ‡∏≠‡∏¢
sorted_idx = np.argsort(feature_importances)[::-1]
sorted_features = [feature_names[i] for i in sorted_idx]
sorted_importances = feature_importances[sorted_idx]

# ‡∏û‡∏¥‡∏°‡∏û‡πå Feature ranking
print("Top 15 Feature ranking (Random Forest):")
for i in range(15):  # 15 ‡∏≠‡∏±‡∏ô‡∏î‡∏±‡∏ö‡πÅ‡∏£‡∏Å
    print(f"  {i+1}. feature {sorted_features[i]} ({sorted_importances[i]:.6f})")

# ‡πÄ‡∏•‡∏∑‡∏≠‡∏Å‡πÄ‡∏â‡∏û‡∏≤‡∏∞ 15 ‡∏≠‡∏±‡∏ô‡∏î‡∏±‡∏ö‡πÅ‡∏£‡∏Å
top_15_features = sorted_features[:15]
top_15_importances = sorted_importances[:15]

# ‡∏û‡∏•‡πá‡∏≠‡∏ï‡∏Å‡∏£‡∏≤‡∏ü bar chart ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö 15 ‡∏≠‡∏±‡∏ô‡∏î‡∏±‡∏ö‡πÅ‡∏£‡∏Å
plt.figure(figsize=(10, 6))  
plt.bar(top_15_features, top_15_importances)
plt.ylabel("Feature Importance")
plt.xlabel("Feature")
plt.title("Feature Importance for Random Forest Regression (Top 15)")
plt.xticks(rotation=90)  # ‡∏´‡∏°‡∏∏‡∏ô labels ‡∏Ç‡∏≠‡∏á‡πÅ‡∏Å‡∏ô x 90 ‡∏≠‡∏á‡∏®‡∏≤
plt.tight_layout() 
plt.show()


# ‡∏Å‡∏£‡∏≤‡∏ü‡∏ô‡∏µ‡πâ‡πÅ‡∏™‡∏î‡∏á‡∏ñ‡∏∂‡∏á‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏Ç‡∏≠‡∏á‡∏ï‡∏±‡∏ß‡πÅ‡∏õ‡∏£‡∏ï‡πà‡∏≤‡∏á‡πÜ (Features) ‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå‡∏î‡πâ‡∏ß‡∏¢‡πÇ‡∏°‡πÄ‡∏î‡∏• Random Forest Regression ‡πÇ‡∏î‡∏¢‡πÅ‡∏™‡∏î‡∏á‡πÄ‡∏â‡∏û‡∏≤‡∏∞ 15 ‡∏≠‡∏±‡∏ô‡∏î‡∏±‡∏ö‡πÅ‡∏£‡∏Å‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏ú‡∏•‡∏°‡∏≤‡∏Å‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î‡∏ï‡πà‡∏≠‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢ 
# 
# **‡∏™‡∏£‡∏∏‡∏õ‡∏ú‡∏•‡∏à‡∏≤‡∏Å‡∏Å‡∏£‡∏≤‡∏ü**
# 
# * **Smoker:** ‡πÄ‡∏õ‡πá‡∏ô‡∏ï‡∏±‡∏ß‡πÅ‡∏õ‡∏£‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏°‡∏≤‡∏Å‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå  ‡πÇ‡∏î‡∏¢‡∏°‡∏µ‡∏Ñ‡πà‡∏≤ Feature Importance ‡∏™‡∏π‡∏á‡∏Å‡∏ß‡πà‡∏≤‡∏ï‡∏±‡∏ß‡πÅ‡∏õ‡∏£‡∏≠‡∏∑‡πà‡∏ô‡πÜ ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÄ‡∏´‡πá‡∏ô‡πÑ‡∏î‡πâ‡∏ä‡∏±‡∏î 
# * **Age:** ‡πÄ‡∏õ‡πá‡∏ô‡∏ï‡∏±‡∏ß‡πÅ‡∏õ‡∏£‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏£‡∏≠‡∏á‡∏•‡∏á‡∏°‡∏≤ ‡πÅ‡∏™‡∏î‡∏á‡∏ß‡πà‡∏≤‡∏≠‡∏≤‡∏¢‡∏∏‡∏Ç‡∏≠‡∏á‡∏ú‡∏π‡πâ‡∏ñ‡∏∑‡∏≠‡∏Å‡∏£‡∏°‡∏ò‡∏£‡∏£‡∏°‡πå‡∏°‡∏µ‡∏ú‡∏•‡∏ï‡πà‡∏≠‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏Ñ‡πà‡∏≤‡∏™‡∏¥‡∏ô‡πÑ‡∏´‡∏° 
# * **BMI:**  ‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡πÉ‡∏ô‡∏£‡∏∞‡∏î‡∏±‡∏ö‡∏´‡∏ô‡∏∂‡πà‡∏á  
# * **Weight, hereditary_diseases_NoDisease:** ‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏•‡∏î‡∏´‡∏•‡∏±‡πà‡∏ô‡∏•‡∏á‡∏°‡∏≤
# * **‡∏ï‡∏±‡∏ß‡πÅ‡∏õ‡∏£‡∏≠‡∏∑‡πà‡∏ô‡πÜ:**  ‡∏ó‡∏µ‡πà‡πÄ‡∏´‡∏•‡∏∑‡∏≠ ‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏Ñ‡πà‡∏≠‡∏ô‡∏Ç‡πâ‡∏≤‡∏á‡∏ô‡πâ‡∏≠‡∏¢  ‡πÇ‡∏î‡∏¢‡∏ö‡∏≤‡∏á‡∏ï‡∏±‡∏ß‡πÅ‡∏õ‡∏£‡πÅ‡∏ó‡∏ö‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏ú‡∏•‡∏ï‡πà‡∏≠‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡πÄ‡∏•‡∏¢
# 
# **‡∏Ç‡πâ‡∏≠‡∏™‡∏±‡∏á‡πÄ‡∏Å‡∏ï:**
# 
# * ‡∏ï‡∏±‡∏ß‡πÅ‡∏õ‡∏£‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏™‡∏π‡∏ö‡∏ö‡∏∏‡∏´‡∏£‡∏µ‡πà (Smoker) ‡∏°‡∏µ‡∏≠‡∏¥‡∏ó‡∏ò‡∏¥‡∏û‡∏•‡∏ï‡πà‡∏≠‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏°‡∏≤‡∏Å‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î  ‡∏ã‡∏∂‡πà‡∏á‡∏™‡∏≠‡∏î‡∏Ñ‡∏•‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à‡∏ó‡∏±‡πà‡∏ß‡πÑ‡∏õ‡∏ó‡∏µ‡πà‡∏ß‡πà‡∏≤‡∏Å‡∏≤‡∏£‡∏™‡∏π‡∏ö‡∏ö‡∏∏‡∏´‡∏£‡∏µ‡πà‡∏™‡πà‡∏á‡∏ú‡∏•‡πÄ‡∏™‡∏µ‡∏¢‡∏ï‡πà‡∏≠‡∏™‡∏∏‡∏Ç‡∏†‡∏≤‡∏û  ‡πÅ‡∏•‡∏∞‡∏≠‡∏≤‡∏à‡∏ô‡∏≥‡πÑ‡∏õ‡∏™‡∏π‡πà‡∏Ñ‡πà‡∏≤‡πÉ‡∏ä‡πâ‡∏à‡πà‡∏≤‡∏¢‡∏î‡πâ‡∏≤‡∏ô‡∏™‡∏∏‡∏Ç‡∏†‡∏≤‡∏û‡∏ó‡∏µ‡πà‡∏™‡∏π‡∏á‡∏Ç‡∏∂‡πâ‡∏ô
# * ‡∏≠‡∏≤‡∏¢‡∏∏ (Age) ‡πÅ‡∏•‡∏∞‡∏î‡∏±‡∏ä‡∏ô‡∏µ‡∏°‡∏ß‡∏•‡∏Å‡∏≤‡∏¢ (BMI) ‡πÄ‡∏õ‡πá‡∏ô‡∏õ‡∏±‡∏à‡∏à‡∏±‡∏¢‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏£‡∏≠‡∏á‡∏•‡∏á‡∏°‡∏≤  ‡∏ã‡∏∂‡πà‡∏á‡∏™‡∏≠‡∏î‡∏Ñ‡∏•‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏™‡∏µ‡πà‡∏¢‡∏á‡∏î‡πâ‡∏≤‡∏ô‡∏™‡∏∏‡∏Ç‡∏†‡∏≤‡∏û‡∏ó‡∏µ‡πà‡πÄ‡∏û‡∏¥‡πà‡∏°‡∏Ç‡∏∂‡πâ‡∏ô‡∏ï‡∏≤‡∏°‡∏≠‡∏≤‡∏¢‡∏∏ ‡πÅ‡∏•‡∏∞‡∏†‡∏≤‡∏ß‡∏∞‡∏ô‡πâ‡∏≥‡∏´‡∏ô‡∏±‡∏Å‡πÄ‡∏Å‡∏¥‡∏ô
# 
# **‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏ä‡∏ô‡πå‡∏Ç‡∏≠‡∏á Feature Importance:**
# 
# * ‡∏ä‡πà‡∏ß‡∏¢‡πÉ‡∏´‡πâ‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à‡∏ß‡πà‡∏≤‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÉ‡∏´‡πâ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏Å‡∏±‡∏ö‡∏ï‡∏±‡∏ß‡πÅ‡∏õ‡∏£‡πÉ‡∏î‡∏°‡∏≤‡∏Å‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î
# * ‡πÄ‡∏õ‡πá‡∏ô‡πÅ‡∏ô‡∏ß‡∏ó‡∏≤‡∏á‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á‡πÇ‡∏°‡πÄ‡∏î‡∏•  ‡πÄ‡∏ä‡πà‡∏ô  ‡∏Å‡∏≤‡∏£‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏ï‡∏±‡∏ß‡πÅ‡∏õ‡∏£‡πÉ‡∏´‡∏°‡πà‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ö‡∏ï‡∏±‡∏ß‡πÅ‡∏õ‡∏£‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏™‡∏π‡∏á 
# * ‡∏ä‡πà‡∏ß‡∏¢‡∏•‡∏î‡∏Ñ‡∏ß‡∏≤‡∏°‡∏ã‡∏±‡∏ö‡∏ã‡πâ‡∏≠‡∏ô‡∏Ç‡∏≠‡∏á‡πÇ‡∏°‡πÄ‡∏î‡∏•  ‡πÇ‡∏î‡∏¢‡∏û‡∏¥‡∏à‡∏≤‡∏£‡∏ì‡∏≤‡∏ï‡∏±‡∏î‡∏ï‡∏±‡∏ß‡πÅ‡∏õ‡∏£‡∏ó‡∏µ‡πà‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏≥‡∏Ñ‡∏±‡∏ç‡∏≠‡∏≠‡∏Å
# 

# # 7.‡∏î‡∏∂‡∏á‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏≠‡∏≠‡∏Å‡πÑ‡∏õ‡πÉ‡∏ä‡πâ‡∏á‡∏≤‡∏ô üíõüíñ

# ## 7.1 ‡∏Å‡∏≤‡∏£‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÇ‡∏°‡πÄ‡∏î‡∏• Random Forest Classifier

# In[83]:


# ‡∏ö‡∏±‡∏ô‡∏ó‡∏∂‡∏Å‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ó‡∏µ‡πà‡∏î‡∏µ‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î‡∏î‡πâ‡∏ß‡∏¢ pickle  
#‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ä‡∏∑‡πà‡∏≠‡∏ß‡πà‡∏≤ final_model_rf

import pickle
filename =' Health_Insurance_Claim_Prediction.pkl'
pickle.dump(final_model_rf, open(filename, 'wb'))


# ### ‡∏Å‡∏≤‡∏£‡πÇ‡∏´‡∏•‡∏î‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏Å‡∏•‡∏±‡∏ö‡∏°‡∏≤‡πÉ‡∏ä‡πâ 

# filename = 'Health_Insurance_Claim_Prediction.sav'
# loaded_model = pickle.load(open(filename, 'rb'))

# ###  ‡∏ß‡∏¥‡∏ò‡∏µ‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏Ñ‡πà‡∏≤‡∏õ‡∏£‡∏∞‡∏Å‡∏±‡∏ô‡∏™‡∏∏‡∏Ç‡∏†‡∏≤‡∏û‡∏ó‡∏µ‡πà‡∏ú‡∏π‡πâ‡∏ñ‡∏∑‡∏≠‡∏Å‡∏£‡∏°‡∏ò‡∏£‡∏£‡∏°‡πå‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡∏£‡πâ‡∏≠‡∏á(Claim Amount)

# #### ‡∏ó‡∏≥‡∏ô‡∏≤‡∏¢‡∏Ñ‡πà‡∏≤
# - Health_Insurance_Claim_Prediction = loaded_model.predict(new_data)
# - ‡πÇ‡∏î‡∏¢ new_data ‡∏ï‡πâ‡∏≠‡∏á‡∏ú‡πà‡∏≤‡∏ô‡∏Å‡∏≤‡∏£‡∏ó‡∏≥ Data processing ‡∏°‡∏≤‡πÅ‡∏•‡πâ‡∏ß
# 
# #### ‡πÅ‡∏™‡∏î‡∏á‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå
# - print("Predictions:", Health_Insurance_Claim_Prediction)
